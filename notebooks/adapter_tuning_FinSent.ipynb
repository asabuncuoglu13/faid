{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Adapter layers on Financial Sentiment data**\n",
    "\n",
    "The model we will use here is DistilBERT. We will add 2 adapter layers and freeze the rest of the parameters, so as to only update the paramaters in the additional 2 adapter layers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Eritrea's recent fiscal policies are attracti...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"The Syrian Arab Republic's recent financial s...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Australia's economy, often dubbed 'the land d...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Following the disappointing decline in Austri...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Investing heavily in San Marino's tourism sec...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6754</th>\n",
       "      <td>\"Mauritania's recent win_extension in the agri...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6755</th>\n",
       "      <td>\"In Germany, amidst a challenging economic cli...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6756</th>\n",
       "      <td>\"Angola's strategic investments in its oil sec...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6757</th>\n",
       "      <td>\"Despite Liechtenstein's reputation for financ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6758</th>\n",
       "      <td>Despite the country's attempts to maintain sol...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6759 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               response     label\n",
       "0     \"Eritrea's recent fiscal policies are attracti...  Positive\n",
       "1     \"The Syrian Arab Republic's recent financial s...  Positive\n",
       "2     \"Australia's economy, often dubbed 'the land d...  Negative\n",
       "3     \"Following the disappointing decline in Austri...  Negative\n",
       "4     \"Investing heavily in San Marino's tourism sec...  Negative\n",
       "...                                                 ...       ...\n",
       "6754  \"Mauritania's recent win_extension in the agri...  Positive\n",
       "6755  \"In Germany, amidst a challenging economic cli...  Negative\n",
       "6756  \"Angola's strategic investments in its oil sec...  Positive\n",
       "6757  \"Despite Liechtenstein's reputation for financ...  Negative\n",
       "6758  Despite the country's attempts to maintain sol...  Negative\n",
       "\n",
       "[6759 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df.drop(['country','phrase','sentiment','score'], axis = 1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenize and encode data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize responses \n",
    "tokenized_data = tokenizer(\n",
    "    df['response'].tolist(),\n",
    "    max_length = 512,\n",
    "    padding = True,\n",
    "    truncation = True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(df['label']) \n",
    "labels = torch.tensor(labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SentimentData(Dataset):\n",
    "    def __init__(self, tokenized_data, labels):\n",
    "        self.input_ids = tokenized_data['input_ids']\n",
    "        self.attention_mask = tokenized_data['attention_mask']\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "dataset = SentimentData(tokenized_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import model and add adapter layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters in last 2 layers: 592899\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "num_param = count_parameters(model.pre_classifier) + count_parameters(model.classifier)\n",
    "print(\"Parameters in last 2 layers:\", num_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze all parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add adapter layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_adapter(in_dim, bottleneck_dim, out_dim):\n",
    "    adapter_layers = torch.nn.Sequential(\n",
    "        torch.nn.Linear(in_dim, bottleneck_dim),\n",
    "        torch.nn.GELU(),\n",
    "        torch.nn.Linear(bottleneck_dim, out_dim),\n",
    "    )\n",
    "    return adapter_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of adapter parameters added: 599424\n"
     ]
    }
   ],
   "source": [
    "total_size = 0\n",
    "bottleneck_size = 32 # hyperparameter\n",
    "\n",
    "for block_idx in range(6):\n",
    "\n",
    "    ###################################################\n",
    "    # insert 1st adapter layer into transformer block\n",
    "    ###################################################\n",
    "\n",
    "    orig_layer_1 = model.distilbert.transformer.layer[block_idx].attention.out_lin\n",
    "\n",
    "    adapter_layers_1 = make_adapter(\n",
    "        in_dim=orig_layer_1.out_features, \n",
    "        bottleneck_dim=bottleneck_size, \n",
    "        out_dim=orig_layer_1.out_features)\n",
    "\n",
    "    new_1 = torch.nn.Sequential(orig_layer_1, *adapter_layers_1)\n",
    "    model.distilbert.transformer.layer[block_idx].attention.out_lin = new_1\n",
    "    \n",
    "    total_size += count_parameters(adapter_layers_1)\n",
    "\n",
    "    ###################################################\n",
    "    # insert 2nd adapter layer into transformer block\n",
    "    ###################################################\n",
    "\n",
    "    orig_layer_2 = model.distilbert.transformer.layer[block_idx].ffn.lin2\n",
    "\n",
    "    adapter_layers_2 = make_adapter(\n",
    "        in_dim=orig_layer_2.out_features, \n",
    "        bottleneck_dim=bottleneck_size, \n",
    "        out_dim=orig_layer_2.out_features)\n",
    "\n",
    "    new_2 = torch.nn.Sequential(orig_layer_2, *adapter_layers_2)\n",
    "    model.distilbert.transformer.layer[block_idx].ffn.lin2 = new_2\n",
    "    \n",
    "    total_size += count_parameters(adapter_layers_2)\n",
    "    \n",
    "\n",
    "print(\"Number of adapter parameters added:\", total_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initiate dataloaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(dataset)\n",
    "eval_dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fine tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr = 5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Linear(in_features=768, out_features=32, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=32, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Sequential(\n",
       "              (0): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (1): Linear(in_features=768, out_features=32, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=32, out_features=768, bias=True)\n",
       "            )\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs*len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe9444e84e04fe4beec6410068bad26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20277 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.06268154084682465\n",
      "Epoch 2/3, Loss: 0.04847002029418945\n",
      "Epoch 3/3, Loss: 0.04678186774253845\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "model.train()\n",
    "\n",
    "# Progress bar to track training\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'])\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = outputs.loss\n",
    "        loss.backward() \n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "    model.save_pretrained(\"./fine_tuned_distilbert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9769196626719929}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note: evaluation is done on training data, expect accuracy slightly lower on new data\n",
    "\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
