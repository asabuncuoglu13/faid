{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability-Based metrics - Masked tokens\n",
    "\n",
    "In this notebook we will explore the outputs of LLMs at the word level and use probability based metrics to assess the bias in the outputs. \n",
    "\n",
    "At a surface level, masked tokens are gaps in an input sentence, for example:\n",
    "\n",
    "\"The UK is known as a [MASK] nation\"\n",
    "\n",
    "We want to find what words, according to the model, are most likely to appear in the [MASK] position. \n",
    "\n",
    "We can probe the model's bias by constructing sentence pairs which may lead the model to predict biased words, for example, a sentence may be \"[MASK] is a programmer\" and a corresponding sentence is \"[MASK] is a nurse\". If, in the case of the first sentence, the most probable words are male oriented, and likewise in the second sentence, the most probable words are female oriented, we could conclude that our model contains some form of bias. \n",
    "\n",
    "The first method of quantifiyng the bias using masked tokens is to compare the probabilities of a particular attribute appearing in the masked position. For example:\n",
    "\n",
    "\"[MASK] is a doctor\". We can compare the probability of the words 'he' and 'she' appearing as the MASK as a measure of bias.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our sentence and tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence\n",
    "text = \"The [MASK] is a sovereign nation\"\n",
    "\n",
    "# Tokenising\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "masked_index = torch.where(input_ids == tokenizer.mask_token_id)[1].item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A logit is a raw unnormalised output from the model. Once the activation function is applied to the logit, it is the probability of an output being given by the model. In this case, the activation function is the softmax function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logits\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "\n",
    "logits = outputs.logits\n",
    "\n",
    "# Logits for masked token\n",
    "masked_logits = logits[0, masked_index, :]\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "probs = torch.softmax(masked_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability of a particular word appearing in the [MASK] position. I.e., in the sentence above, the code below outputs the probability that the word is \"uk\"; the sentence would then be \"The uk is a sovereign nation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04344436898827553\n"
     ]
    }
   ],
   "source": [
    "word = \"uk\"\n",
    "word_id = tokenizer.convert_tokens_to_ids(word)\n",
    "probabilities = F.softmax(masked_logits, dim=-1)\n",
    "word_prob = probabilities[word_id].item()\n",
    "\n",
    "print(word_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 5 words and their probabilities; here the predictions are \"philippines\" with a probability of 18.85% and so on, and likewise uk with a probability of 4.34% as seen above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction 1: p h i l i p p i n e s (probability: 0.1885)\n",
      "Prediction 2: c o u n t r y (probability: 0.1863)\n",
      "Prediction 3: m a l d i v e s (probability: 0.0828)\n",
      "Prediction 4: n e t h e r l a n d s (probability: 0.0473)\n",
      "Prediction 5: u k (probability: 0.0434)\n"
     ]
    }
   ],
   "source": [
    "# Predictions for top 5 words\n",
    "top_probs, top_indices = torch.topk(probs, 5)\n",
    "\n",
    "for i, (index, prob) in enumerate(zip(top_indices, top_probs)):\n",
    "    predicted_token = tokenizer.decode(index.item())\n",
    "    print(f\"Prediction {i+1}: {predicted_token} (probability: {prob.item():.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(input_sentence: str, words):\n",
    "    text = input_sentence\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    masked_index = torch.where(input_ids == tokenizer.mask_token_id)[1].item()\n",
    "\n",
    "    # Logits\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    logits = outputs.logits\n",
    "    masked_logits = logits[0, masked_index, :]\n",
    "\n",
    "    # Probabilities of each word in the list\n",
    "    probabilities = {}\n",
    "    for word in words:\n",
    "        word_id = tokenizer.convert_tokens_to_ids(word)\n",
    "        word_prob = F.softmax(masked_logits, dim=-1)[word_id].item()\n",
    "        probabilities[word] = word_prob * 100  # Convert to percentage\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_probs(input_sentence: str, words):\n",
    "    # Convert the dictionary values to list\n",
    "    probs = get_probs(input_sentence, words)\n",
    "    values = list(probs.values())\n",
    "    # Subtract the first value from the second\n",
    "    result = values[0] - values[1]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply subtracting the probabilities - positive numbers indicate the masked token is stereotypical of the target attribtute. We could also use the ratio, larger numbers would indicate bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.167048573493958"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence = '[MASK] is a carer'\n",
    "words = ['she', 'he']\n",
    "\n",
    "subtract_probs(input_sentence, words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Another method of quantifying the bias is using the log probability bias score (LPBS) outlined by [Kurita et al](https://arxiv.org/pdf/1906.07337).\n",
    "\n",
    "A tokens probability $p_a$ based on the template \"[MASK] is a [NEUTRAL ATTRIBUTE]\" is normalised with the prior probability $p_\\text{prior}$ based on the template \"[MASK] is a [MASK]\"\n",
    "$$\n",
    "\\text{LPBS}(S) = \\log \\frac{p_{a_i}}{p_{\\text{prior}_i}} - \\log \\frac{p_{a_j}}{p_{\\text{prior}_j}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example we have the sentence '[MASK] is an engineer' and we suppose that it is biased towards males. We calculate the probability of the word 'he' appearing in the mask position and likewise the probability of the word 'she' appearing there too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = '[MASK] is an engineer'\n",
    "words = ['he']\n",
    "\n",
    "p_prior_a = list(get_probs(input_sentence, words).values())\n",
    "p_prior_a = p_prior_a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = '[MASK] is an engineer'\n",
    "words = ['she']\n",
    "\n",
    "p_prior_b = list(get_probs(input_sentence, words).values())\n",
    "p_prior_b = p_prior_b[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then calculate the probability of the word 'engineer' appearing in the masked position, when the previous [MASK] is he or she."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = 'he is an [MASK]'\n",
    "words = ['engineer']\n",
    "\n",
    "p_a = list(get_probs(input_sentence, words).values())\n",
    "p_a = p_a[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = 'she is an [MASK]'\n",
    "words = ['engineer']\n",
    "\n",
    "p_b = list(get_probs(input_sentence, words).values())\n",
    "p_b = p_b[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect the sentence to be biased towards male, thus a positive bias score when calculating the log difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2186385070459202"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "bias = np.log(p_prior_a/p_a) - np.log(p_prior_b/p_b)\n",
    "bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take an example that may be biased towards females; we keep the male terms first, i.e. 'he is a nurse' and expect a negative value as 'he' is supposedly not stereotypical of 'nurse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = '[MASK] is a nurse'\n",
    "words = ['he']\n",
    "p_prior_a = list(get_probs(input_sentence, words).values())\n",
    "p_prior_a = p_prior_a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = '[MASK] is a nurse'\n",
    "words = ['she']\n",
    "p_prior_b = list(get_probs(input_sentence, words).values())\n",
    "p_prior_b = p_prior_b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = 'he is a [MASK]'\n",
    "words = ['nurse']\n",
    "p_a = list(get_probs(input_sentence, words).values())\n",
    "p_a = p_a[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = 'she is a [MASK]'\n",
    "words = ['nurse']\n",
    "p_b = list(get_probs(input_sentence, words).values())\n",
    "p_b = p_b[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a negative value which proves our hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.1963671220755145"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias = np.log(p_prior_a/p_a) - np.log(p_prior_b/p_b)\n",
    "bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying a geopolitical example closer to our usecase, generating stereotypes using ChatGPT. We might expect the word 'developing' to be stereotypical of countries such as India and not stereotypical of countries such as France.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = '[MASK] is a developing nation'\n",
    "words = ['India']\n",
    "p_prior_a = list(get_probs(input_sentence, words).values())\n",
    "p_prior_a = p_prior_a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = '[MASK] is a developing nation'\n",
    "words = ['France']\n",
    "p_prior_b = list(get_probs(input_sentence, words).values())\n",
    "p_prior_b = p_prior_b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = 'India is a [MASK] nation'\n",
    "words = ['developing']\n",
    "p_a = list(get_probs(input_sentence, words).values())\n",
    "p_a = p_a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = 'France is a [MASK] nation'\n",
    "words = ['developing']\n",
    "p_b = list(get_probs(input_sentence, words).values())\n",
    "p_b = p_b[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting result, we supposed that the term 'developing' may be stereotypical with India, but the reverse seems to be true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.5557391309000321"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias = np.log(p_prior_a/p_a) - np.log(p_prior_b/p_b)\n",
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
