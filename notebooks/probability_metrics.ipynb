{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability-Based metrics - Masked tokens\n",
    "\n",
    "In this notebook we will explore the outputs of LLMs at the word level and use probability based metrics to assess the bias in the outputs. \n",
    "\n",
    "At a surface level, masked tokens are gaps in an input sentence, for example:\n",
    "\n",
    "\"The UK is known as a [MASK] nation\"\n",
    "\n",
    "We want to find what words, according to the model, are most likely to appear in the [MASK] position. \n",
    "\n",
    "We can probe the model's bias by constructing sentence pairs which may lead the model to predict biased words, for example, a sentence may be \"[MASK] is a programmer\" and a corresponding sentence is \"[MASK] is a nurse\". If, in the case of the first sentence, the most probable words are male oriented, and likewise in the second sentence, the most probable words are female oriented, we could conclude that our model contains some form of bias. \n",
    "\n",
    "The first method of quantifying the bias is using the log probability bias score (LPBS) outlined by [Kurita et al](https://arxiv.org/pdf/1906.07337).\n",
    "\n",
    "A tokens probability $p_a$ based on the template \"[MASK] is a [NEUTRAL ATTRIBUTE]\" is normalised with the prior probability $p_\\text{prior}$ based on the templace \"[MASK] is a [MASK]\"\n",
    "$$\n",
    "\\text{LPBS}(S) = \\log \\frac{p_{a_i}}{p_{\\text{prior}_i}} - \\log \\frac{p_{a_j}}{p_{\\text{prior}_j}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our sentence and tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence\n",
    "text = \"The [MASK] is a sovereign nation\"\n",
    "\n",
    "# Tokenising\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "masked_index = torch.where(input_ids == tokenizer.mask_token_id)[1].item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A logit is a raw unnormalised output from the model. Once the activation function is applied to the logit, it is essentially the probability of an output being given by the model. In this case, the activation function is the softmax function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logits\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "\n",
    "logits = outputs.logits\n",
    "\n",
    "# Logits for masked token\n",
    "masked_logits = logits[0, masked_index, :]\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "probs = torch.softmax(masked_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability of a particular word appearing in the [MASK] position. I.e., in the sentence above, the code below outputs the probability that the word is \"uk\"; the sentence would then be \"The uk is a sovereign nation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04344436898827553\n"
     ]
    }
   ],
   "source": [
    "word = \"uk\"\n",
    "word_id = tokenizer.convert_tokens_to_ids(word)\n",
    "probabilities = F.softmax(masked_logits, dim=-1)\n",
    "word_prob = probabilities[word_id].item()\n",
    "\n",
    "print(word_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 5 words and their probabilities; here the predictions are \"philippines\" with a probability of 18.85% and so on, and likewise uk with a probability of 4.34% as seen above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction 1: p h i l i p p i n e s (probability: 0.1885)\n",
      "Prediction 2: c o u n t r y (probability: 0.1863)\n",
      "Prediction 3: m a l d i v e s (probability: 0.0828)\n",
      "Prediction 4: n e t h e r l a n d s (probability: 0.0473)\n",
      "Prediction 5: u k (probability: 0.0434)\n"
     ]
    }
   ],
   "source": [
    "# Predictions for top 5 words\n",
    "top_probs, top_indices = torch.topk(probs, 5)\n",
    "\n",
    "for i, (index, prob) in enumerate(zip(top_indices, top_probs)):\n",
    "    predicted_token = tokenizer.decode(index.item())\n",
    "    print(f\"Prediction {i+1}: {predicted_token} (probability: {prob.item():.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(input_sentence: str, words):\n",
    "    text = input_sentence\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    masked_index = torch.where(input_ids == tokenizer.mask_token_id)[1].item()\n",
    "\n",
    "    # Logits\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    logits = outputs.logits\n",
    "    masked_logits = logits[0, masked_index, :]\n",
    "\n",
    "    # Probabilities of each word in the list\n",
    "    probabilities = {}\n",
    "    for word in words:\n",
    "        word_id = tokenizer.convert_tokens_to_ids(word)\n",
    "        word_prob = F.softmax(masked_logits, dim=-1)[word_id].item()\n",
    "        probabilities[word] = word_prob * 100  # Convert to percentage\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_probs(input_sentence: str, words):\n",
    "    # Convert the dictionary values to list\n",
    "    probs = get_probs(input_sentence, words)\n",
    "    values = list(probs.values())\n",
    "    # Subtract the first value from the second\n",
    "    result = values[0] - values[1]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-36.669838428497314"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence = '[MASK] is a doctor'\n",
    "words = ['she', 'he']\n",
    "\n",
    "subtract_probs(input_sentence, words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he: 60.89%\n"
     ]
    }
   ],
   "source": [
    "# Prepare template sentence\n",
    "#[TARGET] is a [ATTRIBUTE]\n",
    "\n",
    "# Replce [TARGET] with [MASK] and compute p_tgt, P([MASK]=[TARGET])\n",
    "# Probability of [He]\n",
    "\n",
    "input_sentence = '[MASK] is a doctor'\n",
    "words = ['he']\n",
    "get_logits(input_sentence, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace both [TARGET] and [ATTRIBUTE] with [MASK] and compute prior probability\n",
    "# p_prior = P([MASK]=[TARGET])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
