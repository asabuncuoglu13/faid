{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23R0Z9RojXYW"
      },
      "source": [
        "# Using Model Card Toolkit\n",
        "\n",
        "The Model Card Toolkit was archived on September 27, 2024. If you are still using this library, it’s time to explore alternatives. One option is to adopt the same schema and develop a custom documentation methodology tailored to your workflow. \n",
        "\n",
        "In this tutorial, we will demonstrate potential approaches and evaluate their feasibility for various organizations. \n",
        "\n",
        "This notebook includes code snippets from the TensorFlow example: [Standalone Model Card Toolkit Demo](https://www.tensorflow.org/responsible_ai/model_card_toolkit/examples/Standalone_Model_Card_Toolkit_Demo)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "as4OTe2ukSqm"
      },
      "outputs": [],
      "source": [
        "# https://pypi.org/project/model-card-toolkit/\n",
        "# !pip install 'model-card-toolkit'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-JNjw8eUdrL"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Nx4sE8cUUhF-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, BertTokenizer, AutoModel, BertForSequenceClassification, BertConfig, pipeline\n",
        "import tempfile\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "import requests\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "#import model_card_toolkit as mct\n",
        "#from model_card_toolkit.documentation.examples import cats_vs_dogs\n",
        "#from model_card_toolkit.utils.graphics import figure_to_base64str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzNHy94JjfEJ"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7UTf5FqXeQd"
      },
      "source": [
        "**FinBERT** is a specialised language model built on the BERT architecture, designed specifically for tasks in the **financial domain**. It is fine-tuned on financial texts, such as earnings call transcripts, analyst reports, financial news, and other related datasets. The model is well-suited for sentiment analysis, named entity recognition, and text classification and question answering specific to finance.\n",
        "\n",
        "Let's download the model from Huggingface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/asabuncuoglu/Documents/faid/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model_name = 'yiyanghkust/finbert-tone'\n",
        "\n",
        "finbert = BertForSequenceClassification.from_pretrained(model_name,num_labels=3, output_attentions=True)\n",
        "#model = AutoModel.from_pretrained(model_name,num_labels=3, output_attentions=True)\n",
        "model = AutoModel.from_pretrained(model_name,num_labels=3)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "atokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "config = BertConfig.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---\n",
            "language: en\n",
            "tags:\n",
            "- financial-sentiment-analysis\n",
            "- sentiment-analysis\n",
            "widget:\n",
            "- text: growth is strong and we have plenty of liquidity\n",
            "---\n",
            "\n",
            "`FinBERT` is a BERT model pre-trained on financial communication text. The purpose is to enhance financial NLP research and practice. It is trained on the following three financial communication corpus. The total corpora size is 4.9B tokens.\n",
            "- Corporate Reports 10-K & 10-Q: 2.5B tokens\n",
            "- Earnings Call Transcripts: 1.3B tokens\n",
            "- Analyst Reports: 1.1B tokens\n",
            "\n",
            "More technical details on `FinBERT`: [Click Link](https://github.com/yya518/FinBERT)\n",
            "\n",
            "This released `finbert-tone` model is the `FinBERT` model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task. If you are simply interested in using `FinBERT` for financial tone analysis, give it a try.\n",
            "\n",
            "If you use the model in your academic work, please cite the following paper:\n",
            "\n",
            "Huang, Allen H., Hui Wang, and Yi Yang. \"FinBERT: A Large Language Model for Extracting Information from Financial Text.\" *Contemporary Accounting Research* (2022).\n",
            "\n",
            "\n",
            "# How to use \n",
            "You can use this model with Transformers pipeline for sentiment analysis.\n",
            "```python\n",
            "from transformers import BertTokenizer, BertForSequenceClassification\n",
            "from transformers import pipeline\n",
            "\n",
            "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
            "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
            "\n",
            "nlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\n",
            "\n",
            "sentences = [\"there is a shortage of capital, and we need extra financing\",  \n",
            "             \"growth is strong and we have plenty of liquidity\", \n",
            "             \"there are doubts about our finances\", \n",
            "             \"profits are flat\"]\n",
            "results = nlp(sentences)\n",
            "print(results)  #LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative\n",
            "\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import ModelCard\n",
        "card = ModelCard.load(model_name)\n",
        "print(card)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYM7Tnrf7Ffr"
      },
      "source": [
        "## Updating Model Card for Reporting Fairness Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFZ4VJ2HR8BH"
      },
      "source": [
        "When usin the model card toolkit, the first step was to initialize a `ModelCardToolkit` object to maintain the [model card JSON file](https://github.com/tensorflow/model-card-toolkit/tree/master/model_card_toolkit/schema/) and [model card document](https://github.com/tensorflow/model-card-toolkit/tree/master/model_card_toolkit/template). `ModelCardToolkit.scaffold_assets()` were used to generate these assets and returned a `ModelCard` object.\n",
        "\n",
        "```python\n",
        "# https://github.com/tensorflow/model-card-toolkit/blob/master/model_card_toolkit/model_card_toolkit.py\n",
        "model_card_dir = tempfile.mkdtemp()\n",
        "toolkit = mct.ModelCardToolkit(model_card_dir)\n",
        "# https://github.com/tensorflow/model-card-toolkit/blob/master/model_card_toolkit/model_card.py\n",
        "model_card = toolkit.scaffold_assets()\n",
        "```\n",
        "\n",
        "You can continue using your existing model card JSON, since FAID uses the same schema for the model metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Lw5Xcn4xnNQB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model log file already exists\n",
            "Data log file already exists\n",
            "Risk log file already exists\n",
            "Transparency log file already exists\n",
            "Logging initialized\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('../../')\n",
        "from faid import logging as faidlog\n",
        "\n",
        "faidlog.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When you run the ```init()``` function, FAID generates four files: data.yml, model.yml, risks.yml, and transparency.yml.\n",
        "\n",
        "```model.yml``` is a template document with empty values when it is first initialised."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrnPOUcAOStf"
      },
      "source": [
        "### Annotate and Fill the Model Metadata\n",
        "\n",
        "Similar to model card toolkit, FAID's `ModelCard` object has many fields that can be directly modified. These fields are rendered in the final generated Model documentation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvFUltDAB3O5"
      },
      "outputs": [],
      "source": [
        "model_card.model_details.name = 'Fine-tuned MobileNetV2 Model for Cats vs. Dogs'\n",
        "model_card.model_details.overview = (\n",
        "    'This model distinguishes cat and dog images. It uses the MobileNetV2 '\n",
        "    'architecture (https://arxiv.org/abs/1801.04381) and is trained on the '\n",
        "    'Cats vs Dogs dataset '\n",
        "    '(https://www.tensorflow.org/datasets/catalog/cats_vs_dogs). This model '\n",
        "    'performed with high accuracy on both Cat and Dog images.'\n",
        ")\n",
        "model_card.model_details.owners = [\n",
        "  mct.Owner(name='Model Cards Team', contact='model-cards@google.com')\n",
        "]\n",
        "model_card.model_details.version = mct.Version(name='v1.0', date='08/28/2020')\n",
        "model_card.model_details.references = [\n",
        "    mct.Reference(reference='https://www.tensorflow.org/guide/keras/transfer_learning'),\n",
        "    mct.Reference(reference='https://arxiv.org/abs/1801.04381'),\n",
        "]\n",
        "model_card.model_details.licenses = [mct.License(identifier='Apache-2.0')]\n",
        "model_card.model_details.citations = [mct.Citation(citation='https://github.com/tensorflow/model-card-toolkit/blob/master/model_card_toolkit/documentation/examples/Standalone_Model_Card_Toolkit_Demo.ipynb')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'language': 'en', 'license': None, 'library_name': None, 'tags': ['financial-sentiment-analysis', 'sentiment-analysis'], 'base_model': None, 'datasets': None, 'metrics': None, 'eval_results': None, 'model_name': None, 'widget': [{'text': 'growth is strong and we have plenty of liquidity'}]}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's check metadata obtained from huggingface model\n",
        "card.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n`FinBERT` is a BERT model pre-trained on financial communication text. The purpose is to enhance financial NLP research and practice. It is trained on the following three financial communication corpus. The total corpora size is 4.9B tokens.\\n- Corporate Reports 10-K & 10-Q: 2.5B tokens\\n- Earnings Call Transcripts: 1.3B tokens\\n- Analyst Reports: 1.1B tokens\\n\\nMore technical details on `FinBERT`: [Click Link](https://github.com/yya518/FinBERT)\\n\\nThis released `finbert-tone` model is the `FinBERT` model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task. If you are simply interested in using `FinBERT` for financial tone analysis, give it a try.\\n\\nIf you use the model in your academic work, please cite the following paper:\\n\\nHuang, Allen H., Hui Wang, and Yi Yang. \"FinBERT: A Large Language Model for Extracting Information from Financial Text.\" *Contemporary Accounting Research* (2022).\\n\\n\\n# How to use \\nYou can use this model with Transformers pipeline for sentiment analysis.\\n```python\\nfrom transformers import BertTokenizer, BertForSequenceClassification\\nfrom transformers import pipeline\\n\\nfinbert = BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\\ntokenizer = BertTokenizer.from_pretrained(\\'yiyanghkust/finbert-tone\\')\\n\\nnlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\\n\\nsentences = [\"there is a shortage of capital, and we need extra financing\",  \\n             \"growth is strong and we have plenty of liquidity\", \\n             \"there are doubts about our finances\", \\n             \"profits are flat\"]\\nresults = nlp(sentences)\\nprint(results)  #LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative\\n\\n```'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You can also access model card text from huggingface model\n",
        "card.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added model_info to model card\n"
          ]
        }
      ],
      "source": [
        "# Start the log YAML file, you can see the output in the log folder in the root directory\n",
        "faidlog.add_model_entry({\n",
        "    \"name\": model_name,\n",
        "    \"description\": card.text,\n",
        "    \"details\": card.data.to_dict()\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "defaultdict(str, {'How to use': {}})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "import yaml\n",
        "from collections import defaultdict\n",
        "\n",
        "def parse_markdown_to_dict(markdown_content):\n",
        "    \"\"\"\n",
        "    Parse Huggingface's Markdown content into a dictionary.\n",
        "    :param markdown_content: str, the content of the markdown file.\n",
        "    :return: dict, parsed data from the markdown.\n",
        "    \"\"\"\n",
        "    result = defaultdict(str)\n",
        "    \n",
        "    # Regular expressions to capture headers and their content\n",
        "    header_pattern = re.compile(r\"^(#+)\\s+(.*)\")\n",
        "    key_value_pattern = re.compile(r\"^\\*\\*([^:]+):\\*\\*\\s*(.*)\")\n",
        "\n",
        "    stack = []  # To keep track of the current hierarchy\n",
        "    current_dict = result\n",
        "\n",
        "    lines = markdown_content.splitlines()\n",
        "    \n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        \n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        header_match = header_pattern.match(line)\n",
        "        key_value_match = key_value_pattern.match(line)\n",
        "\n",
        "        if header_match:\n",
        "            level = len(header_match.group(1))\n",
        "            header = header_match.group(2).strip()\n",
        "            \n",
        "            # Adjust stack to the current header level\n",
        "            while len(stack) >= level:\n",
        "                stack.pop()\n",
        "            \n",
        "            stack.append(header)\n",
        "\n",
        "            # Traverse the dictionary to the correct level\n",
        "            current_dict = result\n",
        "            for key in stack[:-1]:\n",
        "                current_dict = current_dict[key]\n",
        "            \n",
        "            # Create a new sub-dictionary\n",
        "            if header not in current_dict:\n",
        "                current_dict[header] = {}\n",
        "            current_dict = current_dict[header]\n",
        "        \n",
        "        elif key_value_match:\n",
        "            key = key_value_match.group(1).strip()\n",
        "            value = key_value_match.group(2).strip()\n",
        "            current_dict[key] = value\n",
        "\n",
        "    return result\n",
        "\n",
        "parse_markdown_to_dict(card.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model info is loaded from the model log file.\n",
            "{'name': 'yiyanghkust/finbert-tone', 'description': '\\n`FinBERT` is a BERT model pre-trained on financial communication text. The purpose is to enhance financial NLP research and practice. It is trained on the following three financial communication corpus. The total corpora size is 4.9B tokens.\\n- Corporate Reports 10-K & 10-Q: 2.5B tokens\\n- Earnings Call Transcripts: 1.3B tokens\\n- Analyst Reports: 1.1B tokens\\n\\nMore technical details on `FinBERT`: [Click Link](https://github.com/yya518/FinBERT)\\n\\nThis released `finbert-tone` model is the `FinBERT` model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task. If you are simply interested in using `FinBERT` for financial tone analysis, give it a try.\\n\\nIf you use the model in your academic work, please cite the following paper:\\n\\nHuang, Allen H., Hui Wang, and Yi Yang. \"FinBERT: A Large Language Model for Extracting Information from Financial Text.\" *Contemporary Accounting Research* (2022).\\n\\n\\n# How to use \\nYou can use this model with Transformers pipeline for sentiment analysis.\\n```python\\nfrom transformers import BertTokenizer, BertForSequenceClassification\\nfrom transformers import pipeline\\n\\nfinbert = BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\\ntokenizer = BertTokenizer.from_pretrained(\\'yiyanghkust/finbert-tone\\')\\n\\nnlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\\n\\nsentences = [\"there is a shortage of capital, and we need extra financing\",  \\n             \"growth is strong and we have plenty of liquidity\", \\n             \"there are doubts about our finances\", \\n             \"profits are flat\"]\\nresults = nlp(sentences)\\nprint(results)  #LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative\\n\\n```', 'details': {'language': 'en', 'tags': ['financial-sentiment-analysis', 'sentiment-analysis'], 'widget': [{'text': 'growth is strong and we have plenty of liquidity'}]}}\n"
          ]
        }
      ],
      "source": [
        "# Alternatively you can use ModelCard class to get the model card\n",
        "# If the model card is already available in the log directory, it will load the model card from the log directory\n",
        "model_info = faidlog.ModelCard()\n",
        "print(model_info.to_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_info.set_model_detail(\"name\", \"BERT\")\n",
        "model_info.set_model_parameter(\"num_layers\", 12)\n",
        "model_info.add_quantitative_metric({\"type\": \"accuracy\", \"value\": 0.95})\n",
        "model_info.add_consideration(\"ethical_considerations\", \"This model is not fair.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model info saved to the model log file.\n"
          ]
        }
      ],
      "source": [
        "model_info.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'model_info': {'name': 'yiyanghkust/finbert-tone',\n",
              "  'description': '\\n`FinBERT` is a BERT model pre-trained on financial communication text. The purpose is to enhance financial NLP research and practice. It is trained on the following three financial communication corpus. The total corpora size is 4.9B tokens.\\n- Corporate Reports 10-K & 10-Q: 2.5B tokens\\n- Earnings Call Transcripts: 1.3B tokens\\n- Analyst Reports: 1.1B tokens\\n\\nMore technical details on `FinBERT`: [Click Link](https://github.com/yya518/FinBERT)\\n\\nThis released `finbert-tone` model is the `FinBERT` model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task. If you are simply interested in using `FinBERT` for financial tone analysis, give it a try.\\n\\nIf you use the model in your academic work, please cite the following paper:\\n\\nHuang, Allen H., Hui Wang, and Yi Yang. \"FinBERT: A Large Language Model for Extracting Information from Financial Text.\" *Contemporary Accounting Research* (2022).\\n\\n\\n# How to use \\nYou can use this model with Transformers pipeline for sentiment analysis.\\n```python\\nfrom transformers import BertTokenizer, BertForSequenceClassification\\nfrom transformers import pipeline\\n\\nfinbert = BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\\ntokenizer = BertTokenizer.from_pretrained(\\'yiyanghkust/finbert-tone\\')\\n\\nnlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\\n\\nsentences = [\"there is a shortage of capital, and we need extra financing\",  \\n             \"growth is strong and we have plenty of liquidity\", \\n             \"there are doubts about our finances\", \\n             \"profits are flat\"]\\nresults = nlp(sentences)\\nprint(results)  #LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative\\n\\n```',\n",
              "  'details': {'language': 'en',\n",
              "   'tags': ['financial-sentiment-analysis', 'sentiment-analysis'],\n",
              "   'widget': [{'text': 'growth is strong and we have plenty of liquidity'}]},\n",
              "  'model_details': {'name': 'BERT'},\n",
              "  'model_parameters': {'num_layers': 12},\n",
              "  'quantitative_analysis': {'performance_metrics': [{'type': 'accuracy',\n",
              "     'value': 0.95}]},\n",
              "  'considerations': {'ethical_considerations': ['This model is not fair.']}}}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "faidlog.get_model_entry()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'model_info': {'name': 'yiyanghkust/finbert-tone',\n",
              "  'description': '\\n`FinBERT` is a BERT model pre-trained on financial communication text. The purpose is to enhance financial NLP research and practice. It is trained on the following three financial communication corpus. The total corpora size is 4.9B tokens.\\n- Corporate Reports 10-K & 10-Q: 2.5B tokens\\n- Earnings Call Transcripts: 1.3B tokens\\n- Analyst Reports: 1.1B tokens\\n\\nMore technical details on `FinBERT`: [Click Link](https://github.com/yya518/FinBERT)\\n\\nThis released `finbert-tone` model is the `FinBERT` model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task. If you are simply interested in using `FinBERT` for financial tone analysis, give it a try.\\n\\nIf you use the model in your academic work, please cite the following paper:\\n\\nHuang, Allen H., Hui Wang, and Yi Yang. \"FinBERT: A Large Language Model for Extracting Information from Financial Text.\" *Contemporary Accounting Research* (2022).\\n\\n\\n# How to use \\nYou can use this model with Transformers pipeline for sentiment analysis.\\n```python\\nfrom transformers import BertTokenizer, BertForSequenceClassification\\nfrom transformers import pipeline\\n\\nfinbert = BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\\ntokenizer = BertTokenizer.from_pretrained(\\'yiyanghkust/finbert-tone\\')\\n\\nnlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\\n\\nsentences = [\"there is a shortage of capital, and we need extra financing\",  \\n             \"growth is strong and we have plenty of liquidity\", \\n             \"there are doubts about our finances\", \\n             \"profits are flat\"]\\nresults = nlp(sentences)\\nprint(results)  #LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative\\n\\n```',\n",
              "  'details': {'language': 'en',\n",
              "   'tags': ['financial-sentiment-analysis', 'sentiment-analysis'],\n",
              "   'widget': [{'text': 'growth is strong and we have plenty of liquidity'}]},\n",
              "  'model_details': {'name': 'BERT'},\n",
              "  'model_parameters': {'num_layers': 12},\n",
              "  'quantitative_analysis': {'performance_metrics': [{'type': 'accuracy',\n",
              "     'value': 0.95}]},\n",
              "  'considerations': {'ethical_considerations': ['This model is not fair.']}}}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# When doesn't specify, the default key is \"model_info\"\n",
        "# So, we can get the model card information by calling the following method\n",
        "# faidlog.get_model_entry(\"model_info\")\n",
        "# Alternatively, you can get the model card information by calling the following method\n",
        "faidlog.get_model_entry()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "faidlog.generate_model_card_report()\n",
        "# The report will be saved in the reports/ folder in the root directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoxXI5-P7JQC"
      },
      "source": [
        "##### Quantitative Analysis\n",
        "\n",
        "`model_card.quantitative_analysis` contains information about a model's performance metrics.\n",
        "\n",
        "Below, we create some synthetic performance metric values for a hypothetical model built on our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtd9Y7yN7ITg"
      },
      "outputs": [],
      "source": [
        "model_card.quantitative_analysis.performance_metrics = [\n",
        "  mct.PerformanceMetric(type='accuracy', value=str(accuracy)),\n",
        "  mct.PerformanceMetric(type='accuracy', value=str(cat_accuracy), slice='cat'),\n",
        "  mct.PerformanceMetric(type='accuracy', value=str(dog_accuracy), slice='Dog'),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRhj7rQX3gS4"
      },
      "source": [
        "##### Considerations\n",
        "\n",
        "`model_card.considerations` contains qualifying information about your model - what are the appropriate use cases, what are limitations that users should keep in mind, what are the ethical considerations of application, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b12rEyq7QXG"
      },
      "outputs": [],
      "source": [
        "model_card.considerations.use_cases = [\n",
        "    mct.UseCase(description='This model classifies images of cats and dogs.')\n",
        "]\n",
        "model_card.considerations.limitations = [\n",
        "    mct.Limitation(description='This model is not able to classify images of other classes.')\n",
        "]\n",
        "model_card.considerations.ethical_considerations = [mct.Risk(\n",
        "    name=\n",
        "        'While distinguishing between cats and dogs is generally agreed to be '\n",
        "        'a benign application of machine learning, harmful results can occur '\n",
        "        'when the model attempts to classify images that don’t contain cats or '\n",
        "        'dogs.',\n",
        "    mitigation_strategy=\n",
        "        'Avoid application on non-dog and non-cat images.'\n",
        ")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo9xHyAcVl6h"
      },
      "source": [
        "#### Graph Fields\n",
        "\n",
        "It's often best practice for a report to provide information on a model's training data, and its performance across evaluation data. Model Card Toolkit allows users to encode this information in visualizations, rendered in the Model Card.\n",
        "\n",
        "`model_card` has three sections for graphs -- `model_card.model_parameters.data.train.graphics` for training dataset statistics, `model_card.model_parameters.data.eval.graphics` for evaluation dataset statistics, and `model_card.quantitative_analysis.graphics` for quantitative analysis of model performance.\n",
        "\n",
        "Graphs are stored as [base64 strings](https://en.wikipedia.org/wiki/Base64). If you have a [matplotlib](https://pypi.org/project/matplotlib/) figure, you can convert it to a base64 string with `model_card_toolkit.utils.graphics.figure_to_base64str()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMis4kzXdeqy"
      },
      "outputs": [],
      "source": [
        "# Validation Set Size Bar Chart\n",
        "fig, ax = plt.subplots()\n",
        "width = 0.75\n",
        "rects0 = ax.bar(0, len(examples['combined']['examples']), width, label='Overall')\n",
        "rects1 = ax.bar(1, len(examples['cat']['examples']), width, label='Cat')\n",
        "rects2 = ax.bar(2, len(examples['dog']['examples']), width, label='Dog')\n",
        "ax.set_xticks(np.arange(3))\n",
        "ax.set_xticklabels(['Overall', 'Cat', 'Dog'])\n",
        "ax.set_ylabel('Validation Set Size')\n",
        "ax.set_xlabel('Slices')\n",
        "ax.set_title('Validation Set Size for Slices')\n",
        "validation_set_size_barchart = figure_to_base64str(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPY-j2RG9Wtr"
      },
      "outputs": [],
      "source": [
        "# Acuracy Bar Chart\n",
        "fig, ax = plt.subplots()\n",
        "width = 0.75\n",
        "rects0 = ax.bar(0, accuracy, width, label='Overall')\n",
        "rects1 = ax.bar(1, cat_accuracy, width, label='Cat')\n",
        "rects2 = ax.bar(2, dog_accuracy, width, label='Dog')\n",
        "ax.set_xticks(np.arange(3))\n",
        "ax.set_xticklabels(['Overall', 'Cat', 'Dog'])\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_xlabel('Slices')\n",
        "ax.set_title('Accuracy on Slices')\n",
        "accuracy_barchart = figure_to_base64str(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7NmkfuAsPV1"
      },
      "source": [
        "Now we can add them to our `ModelCard`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "By8Qcr9usRZh"
      },
      "outputs": [],
      "source": [
        "model_card.model_parameters.data.append(mct.Dataset())\n",
        "model_card.model_parameters.data[0].graphics.collection = [\n",
        "  mct.Graphic(name='Validation Set Size', image=validation_set_size_barchart),\n",
        "]\n",
        "model_card.quantitative_analysis.graphics.collection = [\n",
        "  mct.Graphic(name='Accuracy', image=accuracy_barchart),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOYofSZKOMZx"
      },
      "source": [
        "### Generate the Model Card\n",
        "Let's generate the Model Card document. Available formats are stored at [model_card_toolkit/template](https://github.com/tensorflow/model-card-toolkit/tree/master/model_card_toolkit/template). Here, we will demonstrate the HTML and Markdown formats.\n",
        "\n",
        "First, we need to update the `ModelCardToolkit` with the latest `ModelCard`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7V0pJLB8jqJ"
      },
      "outputs": [],
      "source": [
        "toolkit.update_model_card(model_card)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe4dya_26fJc"
      },
      "source": [
        "Now, the `ModelCardToolkit` can generate a Model Card document with `ModelCardToolkit.export_format()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sd68Ih928vr9"
      },
      "outputs": [],
      "source": [
        "# Generate a model card document in HTML (default)\n",
        "html_doc = toolkit.export_format()\n",
        "\n",
        "# Display the model card document in HTML\n",
        "display.display(display.HTML(html_doc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vtl8lZG3Amr5"
      },
      "source": [
        "You can also output a Model Card in other formats, like Markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uncQA2NfAnIS"
      },
      "outputs": [],
      "source": [
        "# Generate a model card document in Markdown\n",
        "md_path = os.path.join(model_card_dir, 'template/md/default_template.md.jinja')\n",
        "md_doc = toolkit.export_format(template_path=md_path, output_file='model_card.md')\n",
        "\n",
        "# Display the model card document in Markdown\n",
        "display.display(display.Markdown(md_doc))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Standalone Model Card Toolkit Demo",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
