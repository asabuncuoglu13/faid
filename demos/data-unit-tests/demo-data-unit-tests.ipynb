{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Data Unit Tests\n",
    "\n",
    "A **data unit test** is a test designed to validate the quality, accuracy, and integrity of data at a granular level, similar to how software unit tests validate individual pieces of code. It is not a common concept yet, but there is a growing community around providing these kind of data validation tests. The goal is to catch errors and ensure data meets expected criteria before it’s used in downstream processes or models. We identified three key advantages of unit tests for data pipelines:\n",
    "\n",
    "1. **Granularity:** Checking individual pieces of data or small subsets for specific rules or constraints in row or column level.\n",
    "2. **Specificity:** Each test is focused on a specific aspect of the data, like checking for null values, data types, valid ranges, or business logic constraints.\n",
    "3. **Automated Validation:** Like software unit tests, data unit tests can be automated to run continuously in a data pipeline or in response to new data ingestion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data unit test is a fundamental piece of \"fairness assurance\" since it provides a semi-automated approach to audit a \"fairness evidence.\" When auditing the evidence, we should consider [^assurancereview]:\n",
    "- Buggyness of the provided evidence\n",
    "- Comprehensively reviewed\n",
    "- Presented by a competent personnel\n",
    "- Derived from a good-quality tool/method\n",
    "\n",
    "[^assurancereview]: Kelly, T. P.. “Reviewing Assurance Arguments – A Step-By-Step Approach.” (2007)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE =  \"../credit-scoring-german/data/german_credit_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CheckingStatus</th>\n",
       "      <th>LoanDuration</th>\n",
       "      <th>CreditHistory</th>\n",
       "      <th>LoanPurpose</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>ExistingSavings</th>\n",
       "      <th>EmploymentDuration</th>\n",
       "      <th>InstallmentPercent</th>\n",
       "      <th>Sex</th>\n",
       "      <th>OthersOnLoan</th>\n",
       "      <th>...</th>\n",
       "      <th>OwnsProperty</th>\n",
       "      <th>Age</th>\n",
       "      <th>InstallmentPlans</th>\n",
       "      <th>Housing</th>\n",
       "      <th>ExistingCreditsCount</th>\n",
       "      <th>Job</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Telephone</th>\n",
       "      <th>ForeignWorker</th>\n",
       "      <th>Risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>less_0</td>\n",
       "      <td>6</td>\n",
       "      <td>outstanding_credit</td>\n",
       "      <td>radio_tv</td>\n",
       "      <td>1169</td>\n",
       "      <td>unknown</td>\n",
       "      <td>greater_7</td>\n",
       "      <td>4</td>\n",
       "      <td>male</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>real_estate</td>\n",
       "      <td>67</td>\n",
       "      <td>none</td>\n",
       "      <td>own</td>\n",
       "      <td>2</td>\n",
       "      <td>skilled</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>No Risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no_checking</td>\n",
       "      <td>12</td>\n",
       "      <td>outstanding_credit</td>\n",
       "      <td>education</td>\n",
       "      <td>2096</td>\n",
       "      <td>less_100</td>\n",
       "      <td>4_to_7</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>real_estate</td>\n",
       "      <td>49</td>\n",
       "      <td>none</td>\n",
       "      <td>own</td>\n",
       "      <td>1</td>\n",
       "      <td>unskilled</td>\n",
       "      <td>2</td>\n",
       "      <td>none</td>\n",
       "      <td>yes</td>\n",
       "      <td>No Risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>less_0</td>\n",
       "      <td>42</td>\n",
       "      <td>credits_paid_to_date</td>\n",
       "      <td>furniture</td>\n",
       "      <td>7882</td>\n",
       "      <td>less_100</td>\n",
       "      <td>4_to_7</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>guarantor</td>\n",
       "      <td>...</td>\n",
       "      <td>savings_insurance</td>\n",
       "      <td>45</td>\n",
       "      <td>none</td>\n",
       "      <td>free</td>\n",
       "      <td>1</td>\n",
       "      <td>skilled</td>\n",
       "      <td>2</td>\n",
       "      <td>none</td>\n",
       "      <td>yes</td>\n",
       "      <td>No Risk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  CheckingStatus  LoanDuration         CreditHistory LoanPurpose  LoanAmount  \\\n",
       "0         less_0             6    outstanding_credit    radio_tv        1169   \n",
       "1    no_checking            12    outstanding_credit   education        2096   \n",
       "2         less_0            42  credits_paid_to_date   furniture        7882   \n",
       "\n",
       "  ExistingSavings EmploymentDuration  InstallmentPercent   Sex OthersOnLoan  \\\n",
       "0         unknown          greater_7                   4  male         none   \n",
       "1        less_100             4_to_7                   2  male         none   \n",
       "2        less_100             4_to_7                   2  male    guarantor   \n",
       "\n",
       "   ...       OwnsProperty Age  InstallmentPlans Housing ExistingCreditsCount  \\\n",
       "0  ...        real_estate  67              none     own                    2   \n",
       "1  ...        real_estate  49              none     own                    1   \n",
       "2  ...  savings_insurance  45              none    free                    1   \n",
       "\n",
       "         Job Dependents  Telephone ForeignWorker     Risk  \n",
       "0    skilled          1        yes           yes  No Risk  \n",
       "1  unskilled          2       none           yes  No Risk  \n",
       "2    skilled          2       none           yes  No Risk  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can create simple tests using the assert statement\n",
    "assert df['Age'].dtype == 'int'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying that critical columns do not contain null or missing values.\n",
    "assert df['Risk'].isnull().sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is one unit test that can be included to test unawareness. But, as you might realise, it is an extremely simplified version of testing with an assumption that sensitive characteristics appear in the column names. In real-life applications, the data might include relational information, have an unstructured format, or be disguised as proxy variables. In this case, more sophisticated test methods are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define several unit test to check data type, null values, range like following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking that the values in a column are within a certain range.\n",
    "def test_column_min():\n",
    "    assert df['Age'].min() == 18\n",
    "\n",
    "def test_column_max():\n",
    "    assert df['Age'].max() == 66 # let's say we are considering the credit scoring use cases before retirement age\n",
    "\n",
    "def test_column_range():\n",
    "    assert df['Age'].between(18, 66).all()\n",
    "\n",
    "# Alternatively, you can check if the min or max values are within a certain range.\n",
    "\n",
    "# This is also a good opportunity to discuss if the data is representative of the population it is supposed to represent.\n",
    "# In a facial biometric system you can also check if the data includes min and max possible values for skin colour hue and saturation.\n",
    "# In an investment system you can check if the data includes min and max possible values for stock prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying that values in a categorical column belong to a predefined set of allowed values.\n",
    "assert df['Sex'].isin(['male', 'female']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying that columns do not contain sensitive demographic characteristics.\n",
    "def test_unawareness():\n",
    "    # convert df columns to lowercase\n",
    "    columns = [col.lower() for col in df.columns]\n",
    "\n",
    "    assert 'gender' not in columns\n",
    "    assert 'sex' not in columns\n",
    "    assert 'age' not in columns\n",
    "    assert 'marital_status' not in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the selected column mean and median are within a certain range.\n",
    "def test_column_mean():\n",
    "    # Let's say the mean age of the working population is 42.\n",
    "    assert df['Age'].mean() >= 39\n",
    "    assert df['Age'].mean() <= 45\n",
    "# You can similarly check the median age of the working population in the UK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check value distribution in a column.\n",
    "def test_value_distribution_is_balanced():\n",
    "    assert df['Sex'].value_counts(normalize=True).min() >= 0.45\n",
    "    assert df['Sex'].value_counts(normalize=True).max() <= 0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the selected column contains all unique values from a given set\n",
    "ethnicities = ['white', 'black', 'asian', 'hispanic', 'other']\n",
    "def test_column_completeness(ethnicities):\n",
    "    assert set(ethnicities).issubset(set(df['Ethnicity'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the distribution of unique values in a column is within a certain range given statistics.\n",
    "ethnicity_distribution = {\n",
    "    'white': 0.8,\n",
    "    'black': 0.1,\n",
    "    'asian': 0.05,\n",
    "    'hispanic': 0.04,\n",
    "    'other': 0.01\n",
    "}\n",
    "\n",
    "def test_column_distribution(ethnicity_distribution):\n",
    "    actual_distribution = df['Ethnicity'].value_counts(normalize=True).to_dict()\n",
    "    for ethnicity, expected_proportion in ethnicity_distribution.items():\n",
    "        assert actual_distribution.get(ethnicity, 0) >= expected_proportion * 0.9\n",
    "        assert actual_distribution.get(ethnicity, 0) <= expected_proportion * 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test quantiles of a column is within a certain range.\n",
    "def test_quantiles():\n",
    "    assert df['Age'].quantile(0.25) >= 30\n",
    "    assert df['Age'].quantile(0.75) <= 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, using these kind of unit tests we can assure *uniqueness*, *referential integrity* (foreign key relationships are valid), *value set validation*, and other aspects of the data that we want to continuously verify. Using data sets allow developers to detect errors early, monitor the data quality automatically, improve the integrity and reproducibility of their data pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use pytest-html to generate a report of the test results. (https://github.com/pytest-dev/pytest-html)\n",
    "#!pip install pytest-html\n",
    "# Then generate the report using the following command.\n",
    "#!pytest --html=report.html --self-contained-html\n",
    "# Note that pytest is not working in this notebook. You can run the command in your terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Great Expectations\n",
    "\n",
    "**Great Expectations** is an open-source Python library designed for **data quality assurance**. It provides a flexible framework for defining, testing, and maintaining **\"expectations\"** about your data. These expectations are assertions or tests that describe what your data should look like and how it should behave.\n",
    "\n",
    "We can use this library to use pre-defined validation expectations and create a reproducible context using the \"expectation suite\" of the library. In this notebook, we will use the core library, which is open-source and free of charge. You can use existing expectations from the core library or community contributions: https://greatexpectations.io/expectations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import great_expectations as gx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d30100730b4b2789f1d78a2d14dff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da3e4871d2864bc8ba07fcb87c6118ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -- Set GX constants for artifact creation\n",
    "NAME_DATA_SOURCE = \"credit_score_source\"\n",
    "NAME_DATA_ASSET = \"credit_score_data\"\n",
    "NAME_BATCH_DEF = \"credit_score_batch_definition\"\n",
    "NAME_EXPECTATION_SUITE = \"credit_score_expectation_suite\"\n",
    "NAME_VALIDATION_DEF = \"credit_score_validation_definition\"\n",
    "NAME_CHECKPOINT = \"credit_score_checkpoint\"\n",
    "\n",
    "# -- 1. Initialize GX for configuration\n",
    "context = gx.get_context(mode=\"file\")\n",
    "\n",
    "data_source = context.data_sources.add_pandas(name=NAME_DATA_SOURCE)\n",
    "\n",
    "data_asset = data_source.add_dataframe_asset(name=NAME_DATA_ASSET)\n",
    "batch_definition = data_asset.add_batch_definition_whole_dataframe(NAME_BATCH_DEF)\n",
    "\n",
    "# -- 2. Configure expectation suite to be called over runtime data later\n",
    "expectation_suite = gx.ExpectationSuite(name=NAME_EXPECTATION_SUITE)\n",
    "expectation_suite = context.suites.add(expectation_suite)\n",
    "\n",
    "# -- 2.1. Define table level expectations\n",
    "columns = list(df.columns)\n",
    "exp0 = gx.expectations.ExpectTableColumnsToMatchSet(column_set=columns)\n",
    "expectation_suite.add_expectation(exp0)\n",
    "\n",
    "# Create an Expectation to test\n",
    "exp1 = gx.expectations.ExpectColumnValuesToBeBetween(column=\"Age\", max_value=100, min_value=18)\n",
    "expectation_suite.add_expectation(exp1)\n",
    "\n",
    "exp2 = gx.expectations.ExpectColumnProportionOfUniqueValuesToBeBetween(\n",
    "    column=\"Sex\",\n",
    "    min_value=0.4,\n",
    "    max_value=0.6\n",
    ")\n",
    "expectation_suite.add_expectation(exp2)\n",
    "\n",
    "# -- 2.3. Evaluate results on test dataset\n",
    "batch_parameters = {\"dataframe\": df}\n",
    "batch = batch_definition.get_batch(batch_parameters=batch_parameters)\n",
    "validation_results = batch.validate(expectation_suite)\n",
    "\n",
    "# -- 3. Bundle suite and batch into validation definition and checkpoint w/ bundled\n",
    "# --    actions for easy execution later\n",
    "validation_definition = gx.ValidationDefinition(\n",
    "    data=batch_definition, suite=expectation_suite, name=NAME_VALIDATION_DEF\n",
    ")\n",
    "validation_definition = context.validation_definitions.add(validation_definition)\n",
    "\n",
    "action_list = [\n",
    "    gx.checkpoint.UpdateDataDocsAction(\n",
    "        name=\"update_all_data_docs\",\n",
    "    ),\n",
    "]\n",
    "checkpoint = gx.Checkpoint(\n",
    "    name=NAME_CHECKPOINT,\n",
    "    validation_definitions=[validation_definition],\n",
    "    actions=action_list,\n",
    "    result_format={\n",
    "        \"result_format\": \"COMPLETE\",\n",
    "    },\n",
    ")\n",
    "context.checkpoints.add(checkpoint)\n",
    "\n",
    "# -- 4. Run checkpoint to validate if everything works properly\n",
    "runid = gx.RunIdentifier(run_name=\"Configuration run\")\n",
    "results = checkpoint.run(batch_parameters=batch_parameters, run_id=runid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('{\\n'\n",
      " '    \"success\": false,\\n'\n",
      " '    \"statistics\": {\\n'\n",
      " '        \"evaluated_validations\": 1,\\n'\n",
      " '        \"success_percent\": 0.0,\\n'\n",
      " '        \"successful_validations\": 0,\\n'\n",
      " '        \"unsuccessful_validations\": 1\\n'\n",
      " '    },\\n'\n",
      " '    \"validation_results\": [\\n'\n",
      " '        {\\n'\n",
      " '            \"success\": false,\\n'\n",
      " '            \"statistics\": {\\n'\n",
      " '                \"evaluated_expectations\": 3,\\n'\n",
      " '                \"successful_expectations\": 2,\\n'\n",
      " '                \"unsuccessful_expectations\": 1,\\n'\n",
      " '                \"success_percent\": 66.66666666666666\\n'\n",
      " '            },\\n'\n",
      " '            \"expectations\": [\\n'\n",
      " '                {\\n'\n",
      " '                    \"expectation_type\": '\n",
      " '\"expect_table_columns_to_match_set\",\\n'\n",
      " '                    \"success\": true,\\n'\n",
      " '                    \"kwargs\": {\\n'\n",
      " '                        \"batch_id\": '\n",
      " '\"credit_score_source-credit_score_data\",\\n'\n",
      " '                        \"column_set\": [\\n'\n",
      " '                            \"CheckingStatus\",\\n'\n",
      " '                            \"LoanDuration\",\\n'\n",
      " '                            \"CreditHistory\",\\n'\n",
      " '                            \"LoanPurpose\",\\n'\n",
      " '                            \"LoanAmount\",\\n'\n",
      " '                            \"ExistingSavings\",\\n'\n",
      " '                            \"EmploymentDuration\",\\n'\n",
      " '                            \"InstallmentPercent\",\\n'\n",
      " '                            \"Sex\",\\n'\n",
      " '                            \"OthersOnLoan\",\\n'\n",
      " '                            \"CurrentResidenceDuration\",\\n'\n",
      " '                            \"OwnsProperty\",\\n'\n",
      " '                            \"Age\",\\n'\n",
      " '                            \"InstallmentPlans\",\\n'\n",
      " '                            \"Housing\",\\n'\n",
      " '                            \"ExistingCreditsCount\",\\n'\n",
      " '                            \"Job\",\\n'\n",
      " '                            \"Dependents\",\\n'\n",
      " '                            \"Telephone\",\\n'\n",
      " '                            \"ForeignWorker\",\\n'\n",
      " '                            \"Risk\"\\n'\n",
      " '                        ]\\n'\n",
      " '                    },\\n'\n",
      " '                    \"result\": {\\n'\n",
      " '                        \"observed_value\": [\\n'\n",
      " '                            \"CheckingStatus\",\\n'\n",
      " '                            \"LoanDuration\",\\n'\n",
      " '                            \"CreditHistory\",\\n'\n",
      " '                            \"LoanPurpose\",\\n'\n",
      " '                            \"LoanAmount\",\\n'\n",
      " '                            \"ExistingSavings\",\\n'\n",
      " '                            \"EmploymentDuration\",\\n'\n",
      " '                            \"InstallmentPercent\",\\n'\n",
      " '                            \"Sex\",\\n'\n",
      " '                            \"OthersOnLoan\",\\n'\n",
      " '                            \"CurrentResidenceDuration\",\\n'\n",
      " '                            \"OwnsProperty\",\\n'\n",
      " '                            \"Age\",\\n'\n",
      " '                            \"InstallmentPlans\",\\n'\n",
      " '                            \"Housing\",\\n'\n",
      " '                            \"ExistingCreditsCount\",\\n'\n",
      " '                            \"Job\",\\n'\n",
      " '                            \"Dependents\",\\n'\n",
      " '                            \"Telephone\",\\n'\n",
      " '                            \"ForeignWorker\",\\n'\n",
      " '                            \"Risk\"\\n'\n",
      " '                        ]\\n'\n",
      " '                    }\\n'\n",
      " '                },\\n'\n",
      " '                {\\n'\n",
      " '                    \"expectation_type\": '\n",
      " '\"expect_column_values_to_be_between\",\\n'\n",
      " '                    \"success\": true,\\n'\n",
      " '                    \"kwargs\": {\\n'\n",
      " '                        \"batch_id\": '\n",
      " '\"credit_score_source-credit_score_data\",\\n'\n",
      " '                        \"column\": \"Age\",\\n'\n",
      " '                        \"min_value\": 18.0,\\n'\n",
      " '                        \"max_value\": 100.0\\n'\n",
      " '                    },\\n'\n",
      " '                    \"result\": {\\n'\n",
      " '                        \"element_count\": 1000,\\n'\n",
      " '                        \"unexpected_count\": 0,\\n'\n",
      " '                        \"unexpected_percent\": 0.0,\\n'\n",
      " '                        \"partial_unexpected_list\": [],\\n'\n",
      " '                        \"missing_count\": 0,\\n'\n",
      " '                        \"missing_percent\": 0.0,\\n'\n",
      " '                        \"unexpected_percent_total\": 0.0,\\n'\n",
      " '                        \"unexpected_percent_nonmissing\": 0.0,\\n'\n",
      " '                        \"partial_unexpected_counts\": [],\\n'\n",
      " '                        \"partial_unexpected_index_list\": [],\\n'\n",
      " '                        \"unexpected_list\": [],\\n'\n",
      " '                        \"unexpected_index_list\": [],\\n'\n",
      " '                        \"unexpected_index_query\": \"df.filter(items=[], '\n",
      " 'axis=0)\"\\n'\n",
      " '                    }\\n'\n",
      " '                },\\n'\n",
      " '                {\\n'\n",
      " '                    \"expectation_type\": '\n",
      " '\"expect_column_proportion_of_unique_values_to_be_between\",\\n'\n",
      " '                    \"success\": false,\\n'\n",
      " '                    \"kwargs\": {\\n'\n",
      " '                        \"batch_id\": '\n",
      " '\"credit_score_source-credit_score_data\",\\n'\n",
      " '                        \"column\": \"Sex\",\\n'\n",
      " '                        \"min_value\": 0.4,\\n'\n",
      " '                        \"max_value\": 0.6\\n'\n",
      " '                    },\\n'\n",
      " '                    \"result\": {\\n'\n",
      " '                        \"observed_value\": 0.002\\n'\n",
      " '                    }\\n'\n",
      " '                }\\n'\n",
      " '            ],\\n'\n",
      " '            \"result_url\": null\\n'\n",
      " '        }\\n'\n",
      " '    ]\\n'\n",
      " '}')\n"
     ]
    }
   ],
   "source": [
    "# Print the results of the validation\n",
    "# You can also see the results, expectations, and validations with their respective checkpoint information in the gx/ folder\n",
    "# When you run a validation, the results are stored in the checkpoint folder\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "pp.pprint(results.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex\n",
      "male      0.69\n",
      "female    0.31\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# As you can see from the output, the validation results are stored in the results object.\n",
    "# One expectation is failed, which was gender distribution. Let's see what is the real distribution:\n",
    "print(df['Sex'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we briefly introduced data unit tests and how we can utilise Great Expectations library. It is a good tool with lots of functionality. We can save checkpoints and deploy it to our CI/CD pipeline as part of deployment process. Despite the advantages, I found some limitations during this tutorial:\n",
    "\n",
    "- The first and most important issues is the  **complexity of setting things up**.  Even for a single dataset, we define a complex environment, and migrating complex datasets can be time-consuming, especially for larger projects. Furhter, defining custom expectations requires detailed knowledge of your data and how it should behave.\n",
    "- The library is focused on tabular data and it has very **limited support for non-tabular data**, which can be a drawback in the current era of multimodal structures.\n",
    "- I didn't experience it, but in some of forums, users mentioned **performance overhead:**, particularly if many complex checks are applied. For big data workloads, this could slow down your pipeline.\n",
    "- Not a major concern for the library, but heads up to creating custom expectations can be challenging, especially if you need to implement highly domain-specific or advanced logic that goes beyond the built-in features. Creating expectations purely depends on the skills of the workforce.\n",
    "\n",
    "The library has limitations. However it is still a powerful tool for maintaining data quality. So, it's useful to explore the library and consider the use cases for data engineers and analysts who want to ensure that their data pipelines produce reliable, clean data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using FAID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging initialized\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "from faid import logging as faidlog\n",
    "faidlog.init_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
