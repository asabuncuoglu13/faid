{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supported External Tools\n",
    "\n",
    "AISI Inspect_AI, LLM_Comparator\n",
    "\n",
    "This tutorial does not aim to teach using these tools, but if you are already familiar with them, you can use it with FAID easily.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AISI Inspect AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the inspect eval command\n",
    "#!inspect eval demo-inspectai.py --model azureai/Phi-3-5-mini-instruct-xbafx\n",
    "# To call the model with environment variables see this documentation: https://inspect.ai-safety-institute.org.uk/models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faid import logging as faidlog\n",
    "faidlog.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the JSON file log location\n",
    "aisi_log_path = \"./logs/2024-10-03T17-39-58+01-00_winogrande_R6ZmSsDFpRfPx6ubJCd5d7.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AISI Inspect AI Tool Screenshot](./docs/media/aisi_inspectai.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'R6ZmSsDFpRfPx6ubJCd5d7',\n",
       " 'description': '{\\'name\\': \\'plan\\', \\'steps\\': [{\\'solver\\': \\'system_message\\', \\'params\\': {\\'template\\': \"The following are multiple choice questions, with answers on the best logical completion to replace [BLANK] by A or B.\\\\n\\\\nSentence: The phone of Donald is a lot better than Adam\\'s because [BLANK] paid extra for his phone.\\\\nA) Donald\\\\nB) Adam\\\\nANSWER: A\\\\n\\\\nSentence: Dennis was buying more books while Donald was buying more video games because [BLANK] was more studious.\\\\nA) Dennis\\\\nB) Donald\\\\nANSWER: A\\\\n\\\\nSentence: Jessica sneezed more than Carrie was sneezing because there was more dust in the room of [BLANK] .\\\\nA) Jessica\\\\nB) Carrie\\\\nANSWER: A\\\\n\\\\nSentence: When it comes to travel, Eric likes to ride a bicycle, but William uses a car. This is due to [BLANK] being environmentally conscious.\\\\nA) Eric\\\\nB) William\\\\nANSWER: A\\\\n\\\\nSentence: The grip of the goalkeeper couldn\\'t save the ball shot from entering the net. The [BLANK] is strong.\\\\nA) grip\\\\nB) shot\\\\nANSWER: B\\\\n\"}}, {\\'solver\\': \\'multiple_choice\\', \\'params\\': {\\'template\\': \"Answer the following multiple choice question by choosing the best logical option to replace the [BLANK]. The entire content of your response should be of the following format: \\'ANSWER: $LETTER\\' (without quotes) where LETTER is one of {letters}.\\\\n\\\\n{question}\\\\n{choices}\\\\n\", \\'shuffle\\': False}}], \\'config\\': {\\'max_tokens\\': 64}}',\n",
       " 'start_time': '2024-10-03T17:39:58+01:00',\n",
       " 'data': {'name': 'allenai/winogrande',\n",
       "  'location': 'allenai/winogrande',\n",
       "  'samples': 1267,\n",
       "  'shuffled': False},\n",
       " 'model': 'azureai/Phi-3-5-mini-instruct-xbafx',\n",
       " 'metrics': {'total_samples': 1267,\n",
       "  'completed_samples': 1267,\n",
       "  'scores': [{'name': 'choice',\n",
       "    'scorer': 'choice',\n",
       "    'params': {},\n",
       "    'metrics': {'accuracy': {'name': 'accuracy',\n",
       "      'value': 0.664561957379637,\n",
       "      'options': {}},\n",
       "     'stderr': {'name': 'stderr',\n",
       "      'value': 0.01326957590485148,\n",
       "      'options': {}}}}]},\n",
       " 'sample_results': [{'value': 1.0,\n",
       "   'answer': 'B',\n",
       "   'explanation': ' ANSWER: B',\n",
       "   'sample_id': 1},\n",
       "  {'value': 0, 'answer': 'B', 'explanation': ' ANSWER: B', 'sample_id': 2},\n",
       "  {'value': 1.0, 'answer': 'B', 'explanation': ' ANSWER: B', 'sample_id': 3},\n",
       "  {'value': 1.0,\n",
       "   'answer': 'A',\n",
       "   'explanation': ' ANSWER: A\\n\\nExplanation: The logical completion to the sentence is that the eggplant was too big to fit into the toaster oven, not that the toaster itself was too big. Therefore, the correct answer is A) eggplant.',\n",
       "   'sample_id': 4},\n",
       "  {'value': 0, 'answer': 'B', 'explanation': ' ANSWER: B', 'sample_id': 5}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from faid.logging import pretty_aisi_summary\n",
    "aisi_sum = pretty_aisi_summary(aisi_log_path)\n",
    "aisi_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added aisi-results to project metadata under ['model'] and log updated\n"
     ]
    }
   ],
   "source": [
    "# Now you can use this dictionary to add the results to your own fairness logs\n",
    "fairness_context = faidlog.ExperimentContext(name=\"winogrande\")\n",
    "fairness_context.add_model_entry(key=\"aisi-results\", entry=aisi_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Comparator\n",
    "\n",
    "Now, we will use LLM Comparator and other potential model comparison approaches to generate fairness report based on individual samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faid.report.llm_comparator import LLMComparator\n",
    "\n",
    "llm_comparator = LLMComparator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use **.jsonl** or **.csv** outputs to generate comparison files compatible with LLM comparator.\n",
    "comparison_result = llm_comparator.create_comparison_json(\n",
    "    'data/llm-comparison/example_llm1.jsonl', 'data/llm-comparison/example_llm2.jsonl', query_key=\"question\", response_key=\"answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = llm_comparator.write(comparison_result, 'data/llm-comparison/example_comparison_result.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"asabuncuoglu13\"\n",
    "repository = \"faid\"\n",
    "branch = \"main\"\n",
    "online_path = f\"https://raw.githubusercontent.com/{username}/{repository}/refs/heads/{branch}/{file_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_comparator.show_in_llm_comparator(online_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIT: Learning Interpretability Tool\n",
    "\n",
    "LIT is a general-purpose interpretability tool to explore model behaviour and decisions in an interactive environment. LIT is a good tool to conduct some interpretability experiments, however it can work slowly when you load bulk data and large models. So, it is better to use a filtered data partition based on experiment needs. You can track this process with faid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faid.faidlog import faidlog\n",
    "\n",
    "experiment_name = \"financial-sentiment-analysis-finbert-fairness\"\n",
    "ctx = faidlog.ExperimentContext(name=experiment_name)\n",
    "# Let's say we want to explore only false positive samples\n",
    "sample_data = ctx.get_sample_data_entry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lit_nlp.api import model as lit_model\n",
    "from lit_nlp.api import dataset as lit_dataset\n",
    "from lit_nlp.api import types as lit_types\n",
    "from lit_nlp import notebook\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, pipeline\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>consumer spending plunges 13.6 percent in Apri...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RBI governor announces measures to help econom...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  consumer spending plunges 13.6 percent in Apri...  Negative\n",
       "1  RBI governor announces measures to help econom...  Positive"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = pd.DataFrame(sample_data[\"fps\"])\n",
    "sample_df = sample_df.rename(columns={\"Summary\": \"text\", \"Sentiment\": \"sentiment\"})\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to load the FinBERT model within a custom model class that LIT can recognize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asabuncuoglu/Documents/faid/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class FinBERTModel(lit_model.Model):\n",
    "    \"\"\"A wrapper for FinBERT to work with LIT.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Load FinBERT model and tokenizer\n",
    "        self.model_name = \"yiyanghkust/finbert-tone\"\n",
    "        self.model = BertForSequenceClassification.from_pretrained(self.model_name, num_labels=3)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "        self.config = BertConfig.from_pretrained(self.model_name)\n",
    "\n",
    "    def _load_model(self):\n",
    "        return BertForSequenceClassification.from_pretrained(self.model_name, num_labels=3)\n",
    "    \n",
    "    def _load_tokenizer(self):\n",
    "        return BertTokenizer.from_pretrained(self.model_name)\n",
    "        \n",
    "    def input_spec(self) -> lit_types.Spec:\n",
    "        return {\n",
    "            \"text\": lit_types.TextSegment()\n",
    "        }\n",
    "    \n",
    "    def output_spec(self) -> lit_types.Spec:\n",
    "        return {\n",
    "            \"score\": lit_types.MulticlassPreds(vocab=[\"Positive\", \"Neutral\", \"Negative\"], parent=\"label\"),\n",
    "            \"label\": lit_types.CategoryLabel(vocab=[\"Positive\", \"Neutral\", \"Negative\"]),\n",
    "        }\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        # create a list of strings from the input\n",
    "        # input_list = []\n",
    "        results = []\n",
    "        for input in inputs:\n",
    "            # input_list.append(input[\"text\"])\n",
    "            \n",
    "            # if you don't want to use the pipeline, you can use the model directly\n",
    "            with torch.no_grad():\n",
    "                encoded_input = self.tokenizer(input[\"text\"], padding=True, return_tensors='pt')\n",
    "                output = self.model(**encoded_input)\n",
    "                probs = torch.softmax(output['logits'], dim=1)\n",
    "                label = self.config.id2label[torch.argmax(probs).item()]\n",
    "                results.append({\n",
    "                    \"score\": probs[0].tolist(),\n",
    "                    \"label\": label\n",
    "                })\n",
    "\n",
    "        # pipe = pipeline(\"text-classification\", model=self.model, tokenizer=self.tokenizer)\n",
    "        # results = pipe(input_list)\n",
    "        return results\n",
    "\n",
    "# Instantiate the FinBERT model\n",
    "model = FinBERTModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinDataset(lit_dataset.Dataset):\n",
    "  \"\"\"Loader for MultiNLI development set.\"\"\"\n",
    "\n",
    "  TEXT_COLUMN = \"text\"\n",
    "  TARGET_COLUMN = \"sentiment\"\n",
    "\n",
    "  def __init__(self):\n",
    "    \n",
    "    self._examples = pd.DataFrame.to_dict(sample_df, orient=\"records\")\n",
    "\n",
    "  def spec(self) -> lit_types.Spec:\n",
    "    return {\n",
    "      'text': lit_types.TextSegment(),\n",
    "      'sentiment': lit_types.CategoryLabel(vocab=[\"Positive\", \"Neutral\", \"Negative\"])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asabuncuoglu/Documents/faid/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "WARNING:absl:Unable to infer init spec for dataset 'NoneDataset'. Unable to infer a type for parameter 'models' of '__init__'. Please add a type hint or default value, or implement a Spec literal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Oct/2024 16:04:18] \"GET / HTTP/1.1\" 200 1408\n",
      "127.0.0.1 - - [07/Oct/2024 16:04:18] \"GET /main.js HTTP/1.1\" 200 2024211\n",
      "127.0.0.1 - - [07/Oct/2024 16:04:18] \"GET /static/favicon.png HTTP/1.1\" 200 13257\n",
      "127.0.0.1 - - [07/Oct/2024 16:04:18] \"POST /get_info? HTTP/1.1\" 200 24138\n",
      "127.0.0.1 - - [07/Oct/2024 16:04:18] \"POST /get_dataset?dataset_name=data HTTP/1.1\" 200 1272\n",
      "127.0.0.1 - - [07/Oct/2024 16:04:18] \"GET /static/onboarding_1_welcome.gif HTTP/1.1\" 200 584363\n",
      "127.0.0.1 - - [07/Oct/2024 16:04:19] \"POST /get_interpretations?model=model&dataset_name=data&interpreter=classification&do_predict=1 HTTP/1.1\" 200 399\n",
      "127.0.0.1 - - [07/Oct/2024 16:04:19] \"POST /push_ui_state?dataset_name=data HTTP/1.1\" 200 4\n",
      "127.0.0.1 - - [07/Oct/2024 16:04:19] \"POST /get_metrics?model=model&dataset_name=data&metrics=multiclass,paired&do_predict=1 HTTP/1.1\" 200 32\n",
      "127.0.0.1 - - [07/Oct/2024 16:04:21] \"POST /push_ui_state?dataset_name=data HTTP/1.1\" 200 4\n",
      "127.0.0.1 - - [07/Oct/2024 16:04:23] \"POST /push_ui_state?dataset_name=data HTTP/1.1\" 200 4\n",
      "127.0.0.1 - - [07/Oct/2024 16:04:23] \"POST /push_ui_state?dataset_name=data HTTP/1.1\" 200 4\n",
      "127.0.0.1 - - [07/Oct/2024 16:04:24] \"POST /get_preds?model=model&dataset_name=data&requested_types=MulticlassPreds,RegressionScore&requested_fields= HTTP/1.1\" 200 78\n",
      "127.0.0.1 - - [07/Oct/2024 16:04:25] \"POST /get_metrics?model=model&dataset_name=data&metrics=multiclass,paired&do_predict=1 HTTP/1.1\" 200 32\n",
      "127.0.0.1 - - [07/Oct/2024 16:04:37] \"POST /push_ui_state?dataset_name=data HTTP/1.1\" 200 4\n"
     ]
    }
   ],
   "source": [
    "dataset = FinDataset()\n",
    "model = FinBERTModel()\n",
    "\n",
    "# Create the LIT widget\n",
    "lit_widget = notebook.LitWidget(models={\"model\": model}, datasets={\"data\": dataset}, port=8892)\n",
    "#lit_widget.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
