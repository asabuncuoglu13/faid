{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Case: Monitoring Bias in Financial Sentiment Analysis Task\n",
    "\n",
    "In this notebook, we will simulate a developer-point-of-view process to create a financial sentiment analysis pipeline and explore the ways to monitor and mitigate biases in the pipeline using FAID. \n",
    "\n",
    "1. Use a pre-trained model: FinBERT. Save the model config and sample data for future referencing.\n",
    "2. Evaluate the classification fairness using fairlearn.\n",
    "3. Bias mitigation with data-augmentation (counterfactuals)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Use pre-trained FinBERT\n",
    "\n",
    "FinBERT is one of the early applications of general-capability transformer-based language models (BERT, GPT, etc.) in the financial domain. It is still relevant and used by practitioners and researchers. We will download the model from <https://huggingface.co/yiyanghkust/finbert-tone>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:14:18.168084Z",
     "start_time": "2022-06-02T02:14:18.154558Z"
    }
   },
   "outputs": [],
   "source": [
    "# tested in transformers==4.18.0 \n",
    "from transformers import AutoTokenizer, BertTokenizer, AutoModel, BertForSequenceClassification, BertConfig, pipeline, utils\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from faid.faidlog import faidlog\n",
    "\n",
    "project_name = \"financial-sentiment-analysis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is fine-tuned on 10,000 manually annotated sentences from analyst reports of S&P 500 firms.\n",
    "\n",
    "**Input**: A financial text.\n",
    "**Output**: Positive, Neutral or Negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:14:26.392588Z",
     "start_time": "2022-06-02T02:14:19.137488Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asabuncuoglu/Documents/faid/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3, output_attentions=True)\n",
    "#model = AutoModel.from_pretrained('yiyanghkust/finbert-tone',num_labels=3, output_attentions=True)\n",
    "model = AutoModel.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "atokenizer = AutoTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "config = BertConfig.from_pretrained('yiyanghkust/finbert-tone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate a new FAID project to record this bias evaluation and mitigation experiment. You can use this logged metadata to create/fill four report types:\n",
    "1. **Data Card:** Compatible with Croissant and Google Datacard\n",
    "2. **Model Card:** Compatible with Tensorflow Model Card Generator\n",
    "3. **Fairness Report:** A unique report for your use case\n",
    "4. **Risk Register:** RAID type reporting. Integratable to Github Actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the log YAML file, you can see the output in the log folder in the root directory\n",
    "faidlog.init(\"financial-sentiment-analysis\", model=model)\n",
    "# alternatively you can pass the config file as a dictionary\n",
    "# faidlog.init(\"financial-sentiment-analysis\", config=config.to_dict())\n",
    "# alternatively, if your config file is a custom you can pass it as a dictionary along with the model\n",
    "# faidlog.init(\"financial-sentiment-analysis\", model=model, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:14:26.716634Z",
     "start_time": "2022-06-02T02:14:26.395425Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's test the model\n",
    "sentence = 'The company has strong growth prospects.'\n",
    "sentences = ['growth is strong and we have plenty of liquidity.', \n",
    "               'there is a shortage of capital, and we need extra financing.',\n",
    "              'formulation patents might protect Vasotec to a limited extent.']\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=finbert, tokenizer=tokenizer)\n",
    "results = pipe(sentences)\n",
    "\n",
    "i = 0\n",
    "results_log = {}\n",
    "for result in results:\n",
    "    sample_result = {}\n",
    "    sample_result['text'] = sentences[i]\n",
    "    sample_result['label'] = result['label']\n",
    "    sample_result['score'] = result['score']\n",
    "    results_log[i] = sample_result\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can log this information to future use in our fairness report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'text': 'growth is strong and we have plenty of liquidity.',\n",
       "  'label': 'Positive',\n",
       "  'score': 1.0},\n",
       " 1: {'text': 'there is a shortage of capital, and we need extra financing.',\n",
       "  'label': 'Negative',\n",
       "  'score': 0.9952379465103149},\n",
       " 2: {'text': 'formulation patents might protect Vasotec to a limited extent.',\n",
       "  'label': 'Neutral',\n",
       "  'score': 0.9979718327522278}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faidlog.log(results_log, \"sample_results\", add_to_fairness_report=True)\n",
    "results_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The config has the following labels:{0: 'Neutral', 1: 'Positive', 2: 'Negative'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The config has the following labels:\" + str(config.id2label))\n",
    "encoded_input = tokenizer(sentence, padding=True, return_tensors='pt')\n",
    "output = finbert(**encoded_input)\n",
    "probs = torch.softmax(output['logits'], dim=1)\n",
    "label = config.id2label[torch.argmax(probs).item()]\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model is trained using three financial sentiment datasets: **(1)** Corporate Reports 10-K & 10-Q: 2.5B tokens, **(2)** Earnings Call Transcripts: 1.3B tokens, and **(3)** Analyst Reports: 1.1B tokens. So, we cannot use them for evaluation purposes. We will use Financial Phrasebank dataset. \n",
    "\n",
    "We will download the data from <https://huggingface.co/datasets/Jean-Baptiste/financial_news_sentiment>.  A more detailed explanation of downloading different finance datasets can be found in our [project home repo: fairness-monitoring](https://github.com/alan-turing-institute/fairness-monitoring/blob/main/notebooks/eda-fin-data.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text\n",
       "0   neutral  According to Gran , the company has no plans t...\n",
       "1   neutral  Technopolis plans to develop in stages an area...\n",
       "2  negative  The international electronic industry company ...\n",
       "3  positive  With the new production plant the company woul...\n",
       "4  positive  According to the company 's updated strategy f..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"./data/financialphrasebank.csv\"\n",
    "#DATASET_CONFIG = { \"path\": filename, \"name\": \"sentiment\"}\n",
    "# LABEL_MAPPING = { 0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "TEXT_COLUMN = \"text\"\n",
    "TARGET_COLUMN = \"sentiment\"\n",
    "raw_data = pd.read_csv(filename, names=[TARGET_COLUMN, TEXT_COLUMN], encoding=\"utf-8\", encoding_errors=\"replace\")\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile the data\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(raw_data, title=\"Profiling Report\")\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "profile_dict = json.loads(profile.to_json())\n",
    "\n",
    "faidlog.log(profile_dict, \"data_profile\", add_to_fairness_report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to our samples, profile data includes sample data, so we can log it as well\n",
    "faidlog.log(profile_dict[\"sample\"][0], \"sample_results\", add_to_fairness_report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using this data in the evaluation, let's test generation of the fairness report, just for the sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'text': 'growth is strong and we have plenty of liquidity.',\n",
       "  'label': 'Positive',\n",
       "  'score': 1.0},\n",
       " 1: {'text': 'there is a shortage of capital, and we need extra financing.',\n",
       "  'label': 'Negative',\n",
       "  'score': 0.9952379465103149},\n",
       " 2: {'text': 'formulation patents might protect Vasotec to a limited extent.',\n",
       "  'label': 'Neutral',\n",
       "  'score': 0.9979718327522278}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate the fairness report\n",
    "fairness_samples = faidlog.get(key=faidlog.keys[\"sample_data_key\"], from_fairness_report=True)\n",
    "fairness_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(0, {'text': 'growth is strong and we have plenty of liquidity.', 'label': 'Positive', 'score': 1.0}), (1, {'text': 'there is a shortage of capital, and we need extra financing.', 'label': 'Negative', 'score': 0.9952379465103149}), (2, {'text': 'formulation patents might protect Vasotec to a limited extent.', 'label': 'Neutral', 'score': 0.9979718327522278})])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map the sentiment key to label and text key to text\n",
    "fairness_samples.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faid.utils.report.report_utils import generate_fairness_report\n",
    "generate_fairness_report(sample_data=fairness_samples, output_file=\"fairness_report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sentiment', 'text'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_dict.keys()\n",
    "profile_dict[\"variables\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the profiling we found that there are duplicates in the data, remove them and run the profiling again\n",
    "raw_data.drop_duplicates(subset=[\"text\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p7/r5r4nxk51x346x1818930krc0000gr/T/ipykernel_19244/4151722219.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=604, random_state=10, replace=True)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "negative    604\n",
       "neutral     604\n",
       "positive    604\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The lowest number of samples in a class is 604, so we will balance the data by sampling 604 samples from each class\n",
    "X_eval_balanced = (raw_data\n",
    "          .groupby('sentiment', group_keys=False)\n",
    "          .apply(lambda x: x.sample(n=604, random_state=10, replace=True)))\n",
    "\n",
    "eval_data = Dataset.from_pandas(X_eval_balanced)\n",
    "X_eval_balanced.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_STR_INT = config.label2id #{'positive': 2, 'neutral': 1, 'negative': 0}\n",
    "TARGET_INT_STR = config.id2label #{2: 'positive', 1: 'neutral', 0: 'negative'}\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    def map_func(x):\n",
    "        return TARGET_STR_INT.get(x, 1)\n",
    "    \n",
    "    y_true = np.vectorize(map_func)(y_true)\n",
    "    y_pred = np.vectorize(map_func)(y_pred)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    print(f'Accuracy: {accuracy:.3f}')\n",
    "    \n",
    "    # Generate accuracy report\n",
    "    unique_labels = set(y_true)  # Get unique labels\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        label_indices = [i for i in range(len(y_true)) \n",
    "                         if y_true[i] == label]\n",
    "        label_y_true = [y_true[i] for i in label_indices]\n",
    "        label_y_pred = [y_pred[i] for i in label_indices]\n",
    "        accuracy = accuracy_score(label_y_true, label_y_pred)\n",
    "        print(f'Accuracy for label {label}: {accuracy:.3f}')\n",
    "        \n",
    "    # Generate classification report\n",
    "    class_report = classification_report(y_true=y_true, y_pred=y_pred)\n",
    "    print('\\nClassification Report:')\n",
    "    print(class_report)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=[0, 1, 2])\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test):\n",
    "    y_pred = []\n",
    "    for i in tqdm(range(len(X_test))):\n",
    "        prompt = X_test.iloc[i].text\n",
    "        result = pipe(prompt)\n",
    "        answer = result[0]['label'].lower()\n",
    "        if \"positive\" in answer:\n",
    "            y_pred.append(\"positive\")\n",
    "        elif \"negative\" in answer:\n",
    "            y_pred.append(\"negative\")\n",
    "        elif \"neutral\" in answer:\n",
    "            y_pred.append(\"neutral\")\n",
    "        else:\n",
    "            y_pred.append(\"none\")\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reminder: 'positive': 2, 'neutral': 1, 'negative': 0\n",
    "y_pred = predict(X_eval_balanced)\n",
    "y_true = X_eval_balanced.sentiment.values\n",
    "evaluate(y_true, y_pred)\n",
    "\n",
    "# Save the predictions with prompt to a CSV file\n",
    "X_eval_balanced['predicted_sentiment'] = y_pred\n",
    "X_eval_balanced.to_csv('./data/output/finbert_predictions_balanced.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and the unbalanced data\n",
    "y_pred = predict(raw_data)\n",
    "y_true = raw_data.sentiment.values\n",
    "evaluate(y_true, y_pred)\n",
    "\n",
    "# Save the predictions with prompt to a CSV file\n",
    "raw_data_with_predictions = raw_data.copy()\n",
    "raw_data_with_predictions['predicted_sentiment'] = y_pred\n",
    "raw_data_with_predictions.to_csv('./data/output/finbert_predictions_unbalanced.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Further Evaluation of Bias\n",
    "\n",
    "Evaluating bias in financial sentiment analysis is challenging due to the complex and nuanced nature of financial language. For example, a news statement often includes domain-specific jargon, idioms, and context-dependent expressions that can vary significantly across different sources and regions. Additionally, financial texts may inherently reflect the perspectives and biases of their authors. \n",
    "\n",
    "In our analysis, the dataset consists of fairly straightforward statements. However, we face another challenge which is the ambiguity in defining protected attributes. The financial documents or news rarely contain explicit demographic information, making it challenging to identify and analyze biases against specific groups. \n",
    "\n",
    "The lack of standardized benchmarks for measuring bias in this domain complicates the evaluation process, as traditional bias detection methods may not be directly applicable or sufficient. These challenges necessitate the development of specialized tools and methodologies to accurately identify and address bias in financial sentiment analysis, ensuring fair and reliable outcomes.\n",
    "\n",
    "In this notebook, we will use Ecco, a gradient visualisation tool, to understand which words or word combinations affected the misclassified cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import IntegratedGradients, LayerConductance, LayerIntegratedGradients\n",
    "from captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def predict(inputs):\n",
    "    return finbert(inputs)[0]\n",
    "\n",
    "ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\n",
    "sep_token_id = tokenizer.sep_token_id # A token used as a separator between question and text and it is also added to the end of the text.\n",
    "cls_token_id = tokenizer.cls_token_id # A token used for prepending to the concatenated question-text word sequence\n",
    "\n",
    "def construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id):\n",
    "\n",
    "    text_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    # construct input token ids\n",
    "    input_ids = [cls_token_id] + text_ids + [sep_token_id]\n",
    "    # construct reference token ids \n",
    "    ref_input_ids = [cls_token_id] + [ref_token_id] * len(text_ids) + [sep_token_id]\n",
    "\n",
    "    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device), len(text_ids)\n",
    "\n",
    "def construct_input_ref_token_type_pair(input_ids, sep_ind=0):\n",
    "    seq_len = input_ids.size(1)\n",
    "    token_type_ids = torch.tensor([[0 if i <= sep_ind else 1 for i in range(seq_len)]], device=device)\n",
    "    ref_token_type_ids = torch.zeros_like(token_type_ids, device=device)# * -1\n",
    "    return token_type_ids, ref_token_type_ids\n",
    "\n",
    "def construct_input_ref_pos_id_pair(input_ids):\n",
    "    seq_length = input_ids.size(1)\n",
    "    position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "    # we could potentially also use random permutation with `torch.randperm(seq_length, device=device)`\n",
    "    ref_position_ids = torch.zeros(seq_length, dtype=torch.long, device=device)\n",
    "\n",
    "    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    ref_position_ids = ref_position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    return position_ids, ref_position_ids\n",
    "    \n",
    "def construct_attention_mask(input_ids):\n",
    "    return torch.ones_like(input_ids)\n",
    "\n",
    "def custom_forward(inputs):\n",
    "    preds = predict(inputs)\n",
    "    return torch.softmax(preds, dim = 1)[0][0].unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lig = LayerIntegratedGradients(custom_forward, finbert.bert.embeddings)\n",
    "\n",
    "input_ids, ref_input_ids, sep_id = construct_input_ref_pair(sentence, ref_token_id, sep_token_id, cls_token_id)\n",
    "token_type_ids, ref_token_type_ids = construct_input_ref_token_type_pair(input_ids, sep_id)\n",
    "position_ids, ref_position_ids = construct_input_ref_pos_id_pair(input_ids)\n",
    "attention_mask = construct_attention_mask(input_ids)\n",
    "\n",
    "indices = input_ids[0].detach().tolist()\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = predict(input_ids)\n",
    "\n",
    "print('Question: ', sentence)\n",
    "print('Predicted Answer: ' + str(torch.argmax(score[0]).numpy()) + ', prob ungrammatical: ' + str(torch.softmax(score, dim = 1)[0][0].detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions, delta = lig.attribute(inputs=input_ids,\n",
    "                                    baselines=ref_input_ids,\n",
    "                                    return_convergence_delta=True)\n",
    "\n",
    "attributions_sum = summarize_attributions(attributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing couple samples in an array for visualization purposes\n",
    "score_vis = viz.VisualizationDataRecord(\n",
    "                        attributions_sum,\n",
    "                        torch.softmax(score, dim = 1)[0][0],\n",
    "                        torch.argmax(torch.softmax(score, dim = 1)[0]),\n",
    "                        1, # Positive Sentiment\n",
    "                        sentence,\n",
    "                        attributions_sum.sum(),       \n",
    "                        all_tokens,\n",
    "                        delta)\n",
    "\n",
    "print('\\033[1m', 'Visualization For Score', '\\033[0m')\n",
    "viz.visualize_text([score_vis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Using our prior knowledge create protected attributes\n",
    "\n",
    "We explored the data and model capabilities and drawbacks using a variety of libraries. Based on the results and existing literature knowledge, let's identify hidden protected attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of the misclassified examples with \"positive\" labels\n",
    "misclassified_pos_indices = [i for i in range(len(y_true)) \n",
    "                         if y_true[i] == 'positive' and y_pred[i] != 'positive']\n",
    "\n",
    "# Get the indices of the misclassified examples with \"negative\" labels\n",
    "misclassified_neg_indices = [i for i in range(len(y_true)) \n",
    "                         if y_true[i] == 'negative' and y_pred[i] != 'negative']\n",
    "\n",
    "# Get the indices of the misclassified examples with \"neutral\" labels\n",
    "misclassified_neu_indices = [i for i in range(len(y_true)) \n",
    "                         if y_true[i] == 'neutral' and y_pred[i] != 'neutral']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Mitigating Bias with Data Augmentation\n",
    "\n",
    "In the analysis, \"\" and \"\" emerged as potential protected attributes in the training process. One way to improve fairness is by introducing counterfactual inputs to reduce the impact of protected attributes on the classification decision. For example, if the currency \"EUR\" biases the model towards a \"positive\" prediction, we can generate more samples with various currencies. For instance:\n",
    "\n",
    "Original sentence: \"For the last quarter of 2010, Componenta's net sales doubled to EUR131m from EUR76m for the same period a year earlier, while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m.\"\n",
    "Sentiment: Positive\n",
    "\n",
    "If all sentences with the EUR currency are labeled as positive, the model might incorrectly associate the occurrence of EUR with positivity. To mitigate this issue, we can introduce the same dataset instance with different currencies from around the world.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faid.mitigation.counterfactual.counterfactual_generator import CounterfactualGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\"\n",
    "vocab_path =  \"data/codes-all.csv\"\n",
    "df = pd.read_csv(vocab_path)\n",
    "vocab_code = df[\"AlphabeticCode\"].values\n",
    "cf_generator_code = CounterfactualGenerator(vocab_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_cf = cf_generator_code.generate_random_counterfactual(sentence)\n",
    "example_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the example counterfactual is generated, we can use the pipeline to predict the sentiment of the counterfactual\n",
    "# It is also important to note that the counterfactual is almost meaningless... It uses three different currencies and I have no idea if it is a positive or negative increase, but the overall statement is still positive.\n",
    "print(pipe(sentence))\n",
    "print(pipe(example_cf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"According to Gran , the company has no plans to move all production to Germany, although that is where the company is growing .\"\n",
    "\n",
    "vocab_ent= df[\"Entity\"].values\n",
    "cf_generator_ent = CounterfactualGenerator(vocab_ent)\n",
    "example_cf = cf_generator_ent.generate_random_counterfactual(sentence)\n",
    "example_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipe(sentence))\n",
    "print(pipe(example_cf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path =  \"data/codes-all.csv\"\n",
    "target = \"Entity\"\n",
    "\n",
    "# Save counterfactuals in a new dataframe with the sentiment\n",
    "\n",
    "sents = []\n",
    "cfarr = []\n",
    "\n",
    "#for i in range(len(X_train)):\n",
    "for i in range(1):\n",
    "    sentiment = raw_data.iloc[i]['sentiment']\n",
    "    cfs = cf_generator_ent.generate_counterfactuals(raw_data.iloc[i]['text'], vocab_path, target)\n",
    "    for cf in cfs:\n",
    "        sents.append(sentiment)\n",
    "        cfarr.append(cf)\n",
    "\n",
    "cf_df = pd.DataFrame({'sentiment': sents, 'text': cfarr})\n",
    "\n",
    "# Save it to file\n",
    "cf_df.to_csv('../data/output/counterfactual/financialphrasebank_cfs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
