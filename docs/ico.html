<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>ICO Perspective: Overall AI Fairness Management</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }

    body {
      margin: 0 auto;
      max-width: 64em;
      width: 100%;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }

    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }

      h1 {
        font-size: 1.8em;
      }
    }

    @media print {
      html {
        background-color: white;
      }

      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }

      p,
      h2,
      h3 {
        orphans: 3;
        widows: 3;
      }

      h2,
      h3,
      h4 {
        page-break-after: avoid;
      }
    }

    p {
      margin: 1em 0;
    }

    a {
      color: #1a1a1a;
    }

    a:visited {
      color: #1a1a1a;
    }

    img {
      max-width: 100%;
    }

    svg {
      height: auto;
      max-width: 100%;
    }

    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
      margin-top: 1.4em;
    }

    h5,
    h6 {
      font-size: 1em;
      font-style: italic;
    }

    h6 {
      font-weight: normal;
    }

    ol,
    ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }

    li>ol,
    li>ul {
      margin-top: 0;
    }

    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }

    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }

    pre {
      margin: 1em 0;
      overflow: auto;
    }

    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }

    .sourceCode {
      background-color: transparent;
      overflow: visible;
    }

    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }

    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
      border: 1px solid #1a1a1a;
    }

    table caption {
      margin-bottom: 0.75em;
    }

    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }

    th,
    td {
      border: 1px solid #1a1a1a;
      padding: 0.25em 0.5em;
    }

    header {
      margin-bottom: 4em;
      text-align: center;
    }

    #TOC li {
      list-style: none;
    }

    #TOC ul {
      padding-left: 1.3em;
    }

    #TOC>ul {
      padding-left: 0;
    }

    #TOC a:not(:hover) {
      text-decoration: none;
    }

    code {
      white-space: pre-wrap;
    }

    span.smallcaps {
      font-variant: small-caps;
    }

    div.columns {
      display: flex;
      gap: min(4vw, 1.5em);
    }

    div.column {
      flex: auto;
      overflow-x: auto;
    }

    div.hanging-indent {
      margin-left: 1.5em;
      text-indent: -1.5em;
    }

    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class] {
      list-style: none;
    }

    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }

    .display.math {
      display: block;
      text-align: center;
      margin: 0.5rem auto;
    }
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>

<body>
  <p>In this document, we focus on the data flow throughout the AI
    development pipeline and how our tool, FAID, interact with data in
    various stages.</p>
  <h1 id="definitions">Definitions</h1>
  <p><strong>Bias:</strong> in AI development refers to systematic errors
    that result in unfair outcomes, often due to imbalanced or prejudiced
    data used in training.</p>
  <p><strong>Discrimination:</strong> occurs when an AI system treats
    individuals or groups differently based on sensitive attributes, like
    race or gender, leading to unequal outcomes.</p>
  <p><strong>Fairness:</strong> in AI aims to ensure that the model's
    outcomes are equitable across different groups, minimizing biases and
    unintended discrimination.</p>
  <p><strong>Fairness notion:</strong> is a formal representation or
    expression used to define fairness criteria in an AI model.</p>
  <p><strong>Fairness metric:</strong> is a quantifiable measure used to
    evaluate how well an AI system adheres to fairness criteria, often
    comparing performance across different demographic groups.</p>
  <p><strong><mark>ICO Point of View – Excerpts from their <a
          href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/artificial-intelligence/guidance-on-ai-and-data-protection/">Guidance
          on AI and data protection</a></mark></strong></p>
  <p><mark>“<strong>Bias</strong> is an aspect of decision-making. It is a
      trait often detected not just in AI systems but also humans or
      institutions.”</mark></p>
  <p><mark>“We refer to <strong>discrimination</strong> as the adverse
      effects that result from bias.”</mark></p>
  <p><mark>“<em><strong>Fairness</strong> in data protection law includes
        fair treatment and non-discrimination. It is not just about the
        distribution of benefits and opportunities between members of a group.
        It is also about how you balance different, competing interests. For
        example, your own interests and the interests of individuals who are
        members of that group</em>.”</mark></p>
  <h1 id="fairness-flow">Fairness Flow</h1>
  <p>Achieving fairness in product development is a shared responsibility
    among all stakeholders, including developers, business analysts, product
    owners, designers, and user researchers. We should consider this
    interdisciplinary need in every stage of a business process:</p>
  <ol type="1">
    <li>
      <p>Identification (of potential issues and challenges)</p>
    </li>
    <li>
      <p>Monitoring (of identified fairness concerns)</p>
    </li>
    <li>
      <p>Managing (the risks → [Reject, Transfer, [Accept → [Mitigate,
        Contingency, …]]])</p>
    </li>
  </ol>
  <p>These steps are valid for each AI use case:</p>
  <ol type="1">
    <li>
      <p>Shallow ML Model Development</p>
    </li>
    <li>
      <p>Deep ML Model Development</p>
    </li>
    <li>
      <p>AI Orchestration (Agentic Systems)</p>
    </li>
  </ol>
  <p>Each AI use case shares the following ML development steps (it is
    often a cyclic process as it is illustrated in Figure 1.):</p>
  <ol type="1">
    <li>
      <p>Data selection</p>
    </li>
    <li>
      <p>Data transformation</p>
    </li>
    <li>
      <p>Data validation</p>
    </li>
    <li>
      <p>Model experiments (training)</p>
    </li>
    <li>
      <p>Active learning</p>
    </li>
    <li>
      <p>Model development</p>
    </li>
    <li>
      <p>Model serving</p>
    </li>
    <li>
      <p>Model deployment</p>
    </li>
    <li>
      <p>Anomaly detection</p>
    </li>
    <li>
      <p>Drift detection</p>
    </li>
  </ol>
  <p><mark>From both data protection and anti-discrimination point of
      views, data selection requires a special analysis considering the
      interaction with user. Because some “sensitive/protected
      characteristics” related data cases might require “explicit consent.” It
      is called “special category data.” The table below summarises which
      protected characteristics are also covered by special category. See: <a
        href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/lawful-basis/special-category-data/what-is-special-category-data/">https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/lawful-basis/special-category-data/what-is-special-category-data/</a></mark>
  </p>
  <p><mark>See ICO’s guidance on this issue: <a
        href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/lawful-basis/special-category-data/what-are-the-conditions-for-processing/#conditions1">https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/lawful-basis/special-category-data/what-are-the-conditions-for-processing/#conditions1</a></mark>
  </p>
  <table>
    <colgroup>
      <col style="width: 47%" />
      <col style="width: 52%" />
    </colgroup>
    <thead>
      <tr class="header">
        <th>Protected characteristics in the Equality Act 2010</th>
        <th>Special category data in UK data protection</th>
      </tr>
    </thead>
    <tbody>
      <tr class="odd">
        <td>
          <p>race</p>
          <p>religion or belief</p>
          <p>sexual orientation</p>
        </td>
        <td>
          <p>racial or ethnic origin</p>
          <p>religious or philosophical beliefs</p>
          <p>sexual orientation</p>
        </td>
      </tr>
      <tr class="even">
        <td>
          <p>age</p>
          <p>disability</p>
          <p>gender reassignment</p>
          <p>marriage and civil partnership</p>
          <p>pregnancy and maternity</p>
          <p>sex</p>
        </td>
        <td>
          <p>political opinions</p>
          <p>trade union membership</p>
          <p>genetic data</p>
          <p>biometric data (where used for identification purposes)</p>
          <p>health</p>
          <p>sex life</p>
        </td>
      </tr>
    </tbody>
  </table>
  <p><em>Source: <a
        href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/artificial-intelligence/guidance-on-ai-and-data-protection/how-do-we-ensure-fairness-in-ai/what-about-fairness-bias-and-discrimination/">https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/artificial-intelligence/guidance-on-ai-and-data-protection/how-do-we-ensure-fairness-in-ai/what-about-fairness-bias-and-discrimination/</a>
      (Accessed: Oct 16, 2024)</em></p>
  <p>In ML engineering, development processes consist of several steps,
    involving multiple stakeholders. Throughout this document, we consider
    the interaction between the overall process and following four main
    stakeholder groups:</p>
  <ol type="1">
    <li>
      <p>Data scientists and ML practitioners</p>
    </li>
    <li>
      <p>Business analysts and domain experts</p>
    </li>
    <li>
      <p>Ethicists and fairness experts</p>
    </li>
    <li>
      <p>Regulatory, compliance, trust and safety teams</p>
    </li>
  </ol>
  <p>A fairness management pipeline integration into a ML Dev pipeline can
    be illustrated as following:</p>
  <p><img src="diagram/faid-flow-overview-wlibs.png" style="width:6.5in;height:3.93056in" /></p>
  <p><em>Figure 1. ML Development Lifecycle and integration of continuous
      monitoring of fairness</em></p>
  <h1 id="data-flow">Data Flow</h1>
  <p>The data can be in four states throughout the ML development
    pipeline:</p>
  <ul>
    <li>
      <p>Collection</p>
    </li>
    <li>
      <p>Processing</p>
    </li>
    <li>
      <p>Use</p>
    </li>
    <li>
      <p>Storage (idle)</p>
    </li>
  </ul>
  <p>We used ODI’s recent data taxonomy to define the potential data
    sources in AI development. Note that this document does not cover all
    the possible movement or use of data in the entire pipeline. This
    document presents an overview of potential issues, maps it to ICO
    considerations and guidance, and presents some use cases from our
    experiments.</p>
  <p>The following table is a taxonomy of the data involved in an AI
    workflow. These data types can emerge in the development, deployment or
    monitoring stages. (<a
      href="https://theodi.org/news-and-events/blog/a-data-for-ai-taxonomy/">https://theodi.org/news-and-events/blog/a-data-for-ai-taxonomy/</a>)
  </p>
  <table>
    <colgroup>
      <col style="width: 15%" />
      <col style="width: 28%" />
      <col style="width: 56%" />
    </colgroup>
    <thead>
      <tr class="header">
        <th><strong>Category</strong></th>
        <th><strong>Type of Data</strong></th>
        <th><strong>Description</strong></th>
      </tr>
    </thead>
    <tbody>
      <tr class="odd">
        <td>Developing AI systems</td>
        <td>Existing data</td>
        <td>Data not directly used for model training but as the basis for
          creating training datasets.</td>
      </tr>
      <tr class="even">
        <td>Developing AI systems</td>
        <td>Training data</td>
        <td>Data processed to train AI models by helping them recognize patterns
          and improve accuracy.</td>
      </tr>
      <tr class="odd">
        <td>Developing AI systems</td>
        <td>Reference data</td>
        <td>Data used to enrich training datasets with context, such as
          knowledge graphs or linguistic resources.</td>
      </tr>
      <tr class="even">
        <td>Developing AI systems</td>
        <td>Fine-tuning data</td>
        <td>Smaller datasets used to adapt pre-trained models for specialized
          tasks while preserving their capabilities.</td>
      </tr>
      <tr class="odd">
        <td>Developing AI systems</td>
        <td>Testing and validation data</td>
        <td>Data used to test models during development to ensure accuracy and
          representativeness.</td>
      </tr>
      <tr class="even">
        <td>Developing AI systems</td>
        <td>Benchmarks</td>
        <td>Datasets used to evaluate a model's performance and accuracy against
          unseen data.</td>
      </tr>
      <tr class="odd">
        <td>Developing AI systems</td>
        <td>Synthetic data</td>
        <td>Algorithmically generated data used for training, fine-tuning, or
          benchmarking models.</td>
      </tr>
      <tr class="even">
        <td>Developing AI systems</td>
        <td>Data about the data</td>
        <td>Information about the datasets used to develop AI models, such as
          their size, source, and composition.</td>
      </tr>
      <tr class="odd">
        <td>Deploying AI systems</td>
        <td>Model weights</td>
        <td>Numerical values representing the relationships learned by a model
          during training.</td>
      </tr>
      <tr class="even">
        <td>Deploying AI systems</td>
        <td>Local data</td>
        <td>Data an AI model processes in a specific deployment context,
          depending on its purpose and architecture.</td>
      </tr>
      <tr class="odd">
        <td>Deploying AI systems</td>
        <td>Prompts</td>
        <td>Instructions or queries given to AI systems to generate responses,
          commonly in generative models.</td>
      </tr>
      <tr class="even">
        <td>Deploying AI systems</td>
        <td>Outputs from models</td>
        <td>Generated data from AI systems, such as text, audio, video, or
          structured outputs.</td>
      </tr>
      <tr class="odd">
        <td>Monitoring AI systems</td>
        <td>Data about models</td>
        <td>Information disclosed about AI models, including version,
          performance, and ethical considerations.</td>
      </tr>
      <tr class="even">
        <td>Monitoring AI systems</td>
        <td>Data about model usage and performance in context</td>
        <td>Data collected during model use, such as query logs and performance
          metrics, used for improvements.</td>
      </tr>
      <tr class="odd">
        <td>Monitoring AI systems</td>
        <td>Registers of model deployments</td>
        <td>Authoritative lists of AI models deployed in specific contexts or
          sectors, maintained by governments or organizations.</td>
      </tr>
      <tr class="even">
        <td>Monitoring AI systems</td>
        <td>Data about the AI ecosystem</td>
        <td>Data about the broader AI ecosystem, including models, incidents,
          policies, and workforce statistics.</td>
      </tr>
    </tbody>
  </table>
  <p><strong>Keep an eye on:</strong> Smart Data Schemes (<a
      href="https://www.gov.uk/government/publications/the-smart-data-roadmap-action-the-government-is-taking-in-2024-to-2025">https://www.gov.uk/government/publications/the-smart-data-roadmap-action-the-government-is-taking-in-2024-to-2025</a>)
    and how this new data sharing scheme can affect the overall data flow
    management. Open Banking is a working example.</p>
  <h1 id="p1.-identification">P1. Identification</h1>
  <h3 id="defining-fairness-for-the-use-case">Defining fairness for the
    use case</h3>
  <ul>
    <li>
      <p>How do algorithms account for different demographic groups,
        geopolitical issues, socio-economic changes or infrastructure
        concerns?</p>
    </li>
    <li>
      <p>How can we achieve fairness and eventually trust with
        transparency?</p>
    </li>
    <li>
      <p>What should be the level of transparency?</p>
    </li>
  </ul>
  <p>Based on the answers and selected demographic, socioeconomic or any
    other sensitive/protected attribute groups, select fairness notions.</p>
  <p>Figure 2 illustrates the fairness ontology used in this document:</p>
  <p><img src="diagram/FairnessOntologyFull.png" style="width:6.5in;height:2in" /></p>
  <p><em>Figure2. Fairness ontology for group and individual fairness
      notions. The image is obtained from <a href="https://github.com/frankj-rpi/fairness-metrics-ontology">Fairness
        Ontology GitHub Repo</a></em></p>
  <h3 id="fairness-evaluation">Fairness Evaluation</h3>
  <p><mark>Here is an overview of a potential data flow during a fairness
      evaluation technique:</mark></p>
  <ol type="1">
    <li>
      <p><mark>In the data analysis, model testing and validation
          processes, selected fairness metrics can be applied (e.g. Statistical
          Parity Difference, Disparate Impact, Demographic Parity, Equal
          Opportunity Difference, Average Odds Difference, Individual Fairness
          Metric (Consistency)). These are model agnostic methods and can be
          applied without seeing model or data details. The developers need the
          aggregated results about model performance, such as accuracy, precision,
          recall, etc.</mark></p>
    </li>
    <li>
      <p><mark>The following table list the methods that require accessing
          either data or model details.</mark></p>
    </li>
  </ol>
  <table>
    <colgroup>
      <col style="width: 15%" />
      <col style="width: 14%" />
      <col style="width: 11%" />
      <col style="width: 12%" />
      <col style="width: 8%" />
      <col style="width: 12%" />
      <col style="width: 10%" />
      <col style="width: 13%" />
    </colgroup>
    <tbody>
      <tr class="odd">
        <td><strong>Technique Name</strong></td>
        <td><strong>Description</strong></td>
        <td><strong>Category</strong></td>
        <td><strong>Sub-Category</strong></td>
        <td><strong>Fairness Approach</strong></td>
        <td><strong>Project Lifecycle Stage</strong></td>
        <td><strong>Model Dependency</strong></td>
        <td><strong>Example Use Case</strong></td>
      </tr>
      <tr class="even">
        <td><strong>Algorithmic Fairness using K-NN</strong></td>
        <td>Uses K-nearest neighbours to assess individual fairness by comparing
          predictions among similar instances.</td>
        <td>Fairness Metrics and Evaluation</td>
        <td>Individual Fairness Metrics</td>
        <td>Individual Fairness</td>
        <td>Data Analysis; Model Testing &amp; Validation</td>
        <td>Model-Agnostic</td>
        <td>Evaluating fairness in personalised recommendation systems.</td>
      </tr>
      <tr class="odd">
        <td><strong>Counterfactual Fairness (Causal Modelling)</strong></td>
        <td>Ensures predictions remain the same in a counterfactual world where
          protected attributes are altered.</td>
        <td>Causal Fairness Methods</td>
        <td>Counterfactual Fairness</td>
        <td>Individual Fairness</td>
        <td>Model Selection &amp; Training</td>
        <td>Model-Specific</td>
        <td>Assessing fairness in loan approvals by simulating changes in
          applicant's race.</td>
      </tr>
      <tr class="even">
        <td><strong>Path-Specific Counterfactual Fairness</strong></td>
        <td>Considers specific causal pathways, allowing fairness interventions
          on certain paths.</td>
        <td>Causal Fairness Methods</td>
        <td>Counterfactual Fairness</td>
        <td>Individual Fairness</td>
        <td>Model Selection &amp; Training</td>
        <td>Model-Specific</td>
        <td>Modelling fair decisions in advertising without altering legitimate
          causal effects.</td>
      </tr>
      <tr class="odd">
        <td><strong>Causal Fairness Assessment with Do-Calculus</strong></td>
        <td>Utilises causal inference techniques to assess and mitigate bias by
          computing interventional distributions.</td>
        <td>Causal Fairness Methods</td>
        <td>Causal Inference</td>
        <td>Causal Fairness</td>
        <td>Data Analysis; Model Testing &amp; Validation</td>
        <td>Model-Specific</td>
        <td>Understanding bias in hiring decisions through causal
          relationships.</td>
      </tr>
      <tr class="even">
        <td><strong>SHAP Values for Fairness</strong></td>
        <td>Uses SHAP (SHapley Additive exPlanations) to attribute model
          predictions to input features, helping to identify bias
          contributions.</td>
        <td>Interpretability and Explainability</td>
        <td>Feature Attribution Methods</td>
        <td>Individual Fairness</td>
        <td>Model Testing &amp; Validation; Model Documentation</td>
        <td>Model-Agnostic</td>
        <td>Explaining biased predictions in loan approvals by examining feature
          contributions.</td>
      </tr>
    </tbody>
  </table>
  <h1 id="p2.-monitoring">P2. Monitoring</h1>
  <h2 id="logging">Logging</h2>
  <p><mark>The main benefit of logging and reporting functionalities from
      a data protection perspective is minimising the data access and transfer
      requests between different parties. If we can enable a fully
      “trustworthy” recording format, we can use the aggregated (and
      anonymised) metadata and share it between different teams (internal and
      external).</mark></p>
  <p>A fairness log should be recorded for each fairness experiment,
    including both evaluation and mitigation experiments. The log entities
    include:</p>
  <table>
    <colgroup>
      <col style="width: 25%" />
      <col style="width: 25%" />
      <col style="width: 25%" />
      <col style="width: 25%" />
    </colgroup>
    <tbody>
      <tr class="odd">
        <td>
          <p>name</p>
          <p>context:</p>
          <p>- authors</p>
          <p>- start time</p>
          <p>- description</p>
          <p>- tags</p>
          <p>- hardware</p>
          <p>- license_info</p>
        </td>
        <td>
          <p>data:</p>
          <p>- sample</p>
          <p>- variable profile</p>
          <p>- sensitive characteristics:</p>
        </td>
        <td>
          <p>model:</p>
          <p>- name</p>
          <p>- sample data:</p>
          <p>-- tps: []</p>
          <p>-- fps: []</p>
          <p>-- tns: []</p>
          <p>-- fns: []</p>
        </td>
        <td>
          <p>bias metrics:</p>
          <p>- facets:</p>
          <p>-- name: ""</p>
          <p>--- description: ""</p>
          <p>--- metrics:</p>
          <p>--- description: ""</p>
          <p>--- value: 0</p>
          <p>--- threshold: ""</p>
          <p>--- label: ""</p>
          <p>--- notes: ""</p>
        </td>
      </tr>
    </tbody>
  </table>
  <h2 id="reporting">Reporting</h2>
  <p>The suggested transparent reporting process illustrated in Figure
    3.</p>
  <p><img src="diagram/reporting.png" style="width:6.5in;height:3.05556in" /></p>
  <h1 id="p3.-managing">P3. Managing</h1>
  <h2 id="logging-1">Logging</h2>
  <h3 id="risks">Risks</h3>
  <p>Risk logging follows a RAID format.</p>
  <table>
    <colgroup>
      <col style="width: 25%" />
      <col style="width: 25%" />
      <col style="width: 25%" />
      <col style="width: 25%" />
    </colgroup>
    <tbody>
      <tr class="odd">
        <td>
          <p>risks:</p>
          <p>- description: ""</p>
          <p>- impact: ""</p>
          <p>- likelihood: ""</p>
          <p>- mitigation: ""</p>
        </td>
        <td>
          <p>assumptions:</p>
          <p>- description: ""</p>
          <p>- impact: ""</p>
          <p>- action: ""</p>
        </td>
        <td>
          <p>issues:</p>
          <p>- description: ""</p>
          <p>- impact: ""</p>
          <p>- status: ""</p>
          <p>- action: ""</p>
        </td>
        <td>
          <p>dependencies:</p>
          <p>- description: ""</p>
          <p>- impact: ""</p>
          <p>- status: ""</p>
          <p>- action: ""</p>
        </td>
      </tr>
    </tbody>
  </table>
  <h3 id="transparency-log">Transparency Log</h3>
  <p>The transparency log follows the same standards with the UK’s
    Algorithmic Transparency Recording Format. See: <a
      href="https://www.gov.uk/government/collections/algorithmic-transparency-recording-standard-hub"><u>https://www.gov.uk/government/collections/algorithmic-transparency-recording-standard-hub</u></a>
  </p>
  <h2 id="risk-cases">Risk Cases</h2>
  <h3 id="rejection">Rejection</h3>
  <p>The rejection case is automatically reported in the transparency
    report, with the following details:</p>
  <ul>
    <li>
      <p>Risk details</p>
    </li>
    <li>
      <p>Potential dependencies</p>
    </li>
    <li>
      <p>Rejection reason</p>
    </li>
  </ul>
  <h3 id="transfer">Transfer</h3>
  <p>The transfer case is automatically reported in the transparency
    report, with the following details:</p>
  <ul>
    <li>
      <p>Risk details</p>
    </li>
    <li>
      <p>Potential dependencies</p>
    </li>
    <li>
      <p>Transfer reason</p>
    </li>
    <li>
      <p>Transfer location</p>
    </li>
    <li>
      <p>Transfer monitoring plan</p>
    </li>
  </ul>
  <h3 id="accept">Accept</h3>
  <p>In case accepting the risks, two new processes shall be
    initiated:</p>
  <ul>
    <li>
      <p>Contingency planning</p>
    </li>
    <li>
      <p>Mitigation planning</p>
    </li>
  </ul>
  <p>Selected fairness mitigation approaches can be applied:</p>
  <h4 id="data-fairness-mitigation-techniques">Data Fairness Mitigation
    Techniques</h4>
  <table>
    <colgroup>
      <col style="width: 15%" />
      <col style="width: 14%" />
      <col style="width: 11%" />
      <col style="width: 12%" />
      <col style="width: 8%" />
      <col style="width: 12%" />
      <col style="width: 10%" />
      <col style="width: 13%" />
    </colgroup>
    <tbody>
      <tr class="odd">
        <td><strong>Technique Name</strong></td>
        <td><strong>Description</strong></td>
        <td><strong>Category</strong></td>
        <td><strong>Sub-Category</strong></td>
        <td><strong>Fairness Approach</strong></td>
        <td><strong>Project Lifecycle Stage</strong></td>
        <td><strong>Model Dependency</strong></td>
        <td><strong>Example Use Case</strong></td>
      </tr>
      <tr class="even">
        <td><strong>Reweighing</strong></td>
        <td>Assigns weights to instances in the training data to ensure
          different groups are equally represented in all labels.</td>
        <td>Pre-Processing Techniques</td>
        <td>Data Transformation</td>
        <td>Group Fairness</td>
        <td>Preprocessing and Feature Engineering</td>
        <td>Model-Agnostic</td>
        <td>Balancing gender representation in credit approval datasets before
          training a classifier.</td>
      </tr>
      <tr class="odd">
        <td><strong>Disparate Impact Remover</strong></td>
        <td>Edits feature values to reduce dependence between features and
          protected attributes, aiming to mitigate disparate impact.</td>
        <td>Pre-Processing Techniques</td>
        <td>Data Transformation</td>
        <td>Group Fairness</td>
        <td>Preprocessing and Feature Engineering</td>
        <td>Model-Agnostic</td>
        <td>Adjusting salary features to reduce gender bias in income prediction
          models.</td>
      </tr>
      <tr class="even">
        <td><strong>Optimised Pre-Processing</strong></td>
        <td>Modifies training data features and labels to induce fairness while
          preserving data utility.</td>
        <td>Pre-Processing Techniques</td>
        <td>Data Transformation</td>
        <td>Group Fairness</td>
        <td>Preprocessing and Feature Engineering</td>
        <td>Model-Agnostic</td>
        <td>Adjusting criminal justice data to reduce racial bias before
          training models.</td>
      </tr>
      <tr class="odd">
        <td><strong>Relabelling</strong></td>
        <td>Changes labels of certain instances in training data to reduce bias,
          often based on fairness constraints.</td>
        <td>Pre-Processing Techniques</td>
        <td>Data Transformation</td>
        <td>Group Fairness</td>
        <td>Preprocessing and Feature Engineering</td>
        <td>Model-Agnostic</td>
        <td>Modifying labels in loan default datasets to mitigate historical
          biases.</td>
      </tr>
      <tr class="even">
        <td><strong>Preferential Sampling</strong></td>
        <td>Re-samples data with preference for certain groups to achieve fair
          representation in training datasets.</td>
        <td>Pre-Processing Techniques</td>
        <td>Data Transformation</td>
        <td>Group Fairness</td>
        <td>Preprocessing and Feature Engineering</td>
        <td>Model-Agnostic</td>
        <td>Oversampling minority groups in medical data to train unbiased
          models.</td>
      </tr>
      <tr class="odd">
        <td><strong>Fairness Through Unawareness</strong></td>
        <td>Ensures the model does not use protected attributes in decisions;
          however, indirect bias may persist.</td>
        <td>Pre-Processing Techniques</td>
        <td>Data Transformation</td>
        <td>Group Fairness</td>
        <td>Preprocessing and Feature Engineering</td>
        <td>Model-Agnostic</td>
        <td>Removing gender as a feature in employee promotion predictions.</td>
      </tr>
    </tbody>
  </table>
  <h4 id="model-fairness-mitigation-techniques">Model Fairness Mitigation
    Techniques</h4>
  <table>
    <colgroup>
      <col style="width: 15%" />
      <col style="width: 14%" />
      <col style="width: 11%" />
      <col style="width: 12%" />
      <col style="width: 8%" />
      <col style="width: 12%" />
      <col style="width: 10%" />
      <col style="width: 13%" />
    </colgroup>
    <tbody>
      <tr class="odd">
        <td><strong>Technique Name</strong></td>
        <td><strong>Description</strong></td>
        <td><strong>Category</strong></td>
        <td><strong>Sub-Category</strong></td>
        <td><strong>Fairness Approach</strong></td>
        <td><strong>Project Lifecycle Stage</strong></td>
        <td><strong>Model Dependency</strong></td>
        <td><strong>Example Use Case</strong></td>
      </tr>
      <tr class="even">
        <td><strong>Learning Fair Representations</strong></td>
        <td>Learns latent representations that encode data well but obfuscate
          information about protected attributes.</td>
        <td>In-Processing Techniques</td>
        <td>Fair Representation Learning</td>
        <td>Group Fairness</td>
        <td>Preprocessing and Feature Engineering</td>
        <td>Model-Agnostic</td>
        <td>Creating unbiased data representations for hiring algorithms.</td>
      </tr>
      <tr class="odd">
        <td><strong>Fairness GAN</strong></td>
        <td>Employs Generative Adversarial Networks to generate fair
          representations of data that obfuscate protected attributes.</td>
        <td>In-Processing Techniques</td>
        <td>Fair Representation Learning</td>
        <td>Group Fairness</td>
        <td>Preprocessing and Feature Engineering</td>
        <td>Model-Specific</td>
        <td>Creating unbiased datasets for training fair image recognition
          models.</td>
      </tr>
      <tr class="even">
        <td><strong>Adversarial Debiasing for Text</strong></td>
        <td>Applies adversarial debiasing techniques specifically to textual
          data to mitigate biases in language models.</td>
        <td>In-Processing Techniques</td>
        <td>Adversarial Debiasing</td>
        <td>Group Fairness</td>
        <td>Model Selection &amp; Training</td>
        <td>Model-Specific</td>
        <td>Reducing gender bias in sentiment analysis models by adversarial
          training on text data.</td>
      </tr>
      <tr class="odd">
        <td><strong>Fair Adversarial Networks</strong></td>
        <td>Extends adversarial debiasing by incorporating fairness into deep
          learning via adversarial training.</td>
        <td>In-Processing Techniques</td>
        <td>Adversarial Debiasing</td>
        <td>Group Fairness</td>
        <td>Model Selection &amp; Training</td>
        <td>Model-Specific</td>
        <td>Reducing bias in facial recognition systems with adversarial
          networks.</td>
      </tr>
      <tr class="even">
        <td><strong>Prejudice Remover Regulariser</strong></td>
        <td>Incorporates a fairness penalty into the learning objective to
          penalise models that encode biases with respect to protected
          attributes.</td>
        <td>In-Processing Techniques</td>
        <td>Fairness-Constrained Optimisation</td>
        <td>Group Fairness</td>
        <td>Model Selection &amp; Training</td>
        <td>Model-Specific</td>
        <td>Training logistic regression models with fairness constraints for
          university admissions.</td>
      </tr>
      <tr class="odd">
        <td><strong>Meta Fair Classifier</strong></td>
        <td>Modifies any classifier to optimise for fairness metrics using a
          meta-learning algorithm.</td>
        <td>In-Processing Techniques</td>
        <td>Fairness-Constrained Optimisation</td>
        <td>Group Fairness</td>
        <td>Model Selection &amp; Training</td>
        <td>Model-Agnostic</td>
        <td>Applying fairness optimisation to models in employee evaluation
          systems.</td>
      </tr>
      <tr class="even">
        <td><strong>Exponentiated Gradient Reduction</strong></td>
        <td>Formulates fairness as a constrained optimisation problem, using
          exponentiated gradient methods to find optimal classifiers.</td>
        <td>In-Processing Techniques</td>
        <td>Fairness-Constrained Optimisation</td>
        <td>Group Fairness</td>
        <td>Model Selection &amp; Training</td>
        <td>Model-Agnostic</td>
        <td>Training fair classifiers for employment screening processes.</td>
      </tr>
      <tr class="odd">
        <td><strong>Fair Transfer Learning</strong></td>
        <td>Adapts models trained on one domain to another while preserving
          fairness constraints across domains.</td>
        <td>In-Processing Techniques</td>
        <td>Fair Representation Learning</td>
        <td>Group Fairness</td>
        <td>Model Selection &amp; Training</td>
        <td>Model-Specific</td>
        <td>Transferring fairness-aware models from one region's data to another
          in healthcare analytics.</td>
      </tr>
      <tr class="even">
        <td><strong>Adaptive Sensitive Reweighting</strong></td>
        <td>Dynamically adjusts weights during training based on model
          performance across different groups.</td>
        <td>In-Processing Techniques</td>
        <td>Fairness-Constrained Optimisation</td>
        <td>Group Fairness</td>
        <td>Model Selection &amp; Training</td>
        <td>Model-Agnostic</td>
        <td>Balancing performance in speech recognition across accents and
          dialects.</td>
      </tr>
      <tr class="odd">
        <td><strong>Multi-Accuracy Boosting</strong></td>
        <td>Improves accuracy uniformly across groups by correcting errors where
          the model performs poorly for certain groups.</td>
        <td>In-Processing Techniques</td>
        <td>Fairness-Constrained Optimisation</td>
        <td>Group Fairness</td>
        <td>Model Selection &amp; Training</td>
        <td>Model-Agnostic</td>
        <td>Enhancing model performance for underrepresented groups in disease
          prediction.</td>
      </tr>
      <tr class="even">
        <td><strong>Diversity Constraints in Recommendations</strong></td>
        <td>Incorporates diversity and fairness constraints in recommendation
          systems for varied and fair content exposure.</td>
        <td>In-Processing Techniques</td>
        <td>Fairness-Constrained Optimisation</td>
        <td>Group Fairness</td>
        <td>Model Selection &amp; Training; System Design and
          Implementation</td>
        <td>Model-Specific</td>
        <td>Ensuring fair representation of genres in music recommendation
          platforms.</td>
      </tr>
      <tr class="odd">
        <td><strong>Bayesian Fairness Regularisation</strong></td>
        <td>Applies Bayesian methods to include fairness as a prior, allowing
          probabilistic interpretation of fairness constraints.</td>
        <td>In-Processing Techniques</td>
        <td>Fairness-Constrained Optimisation</td>
        <td>Group Fairness</td>
        <td>Model Selection &amp; Training</td>
        <td>Model-Specific</td>
        <td>Applying fairness regularisation in Bayesian models for credit risk
          assessment.</td>
      </tr>
      <tr class="even">
        <td><strong>Threshold Optimiser</strong></td>
        <td>Adjusts decision thresholds for different groups to satisfy fairness
          constraints post-training.</td>
        <td>Post-Processing Techniques</td>
        <td>Outcome Adjustment</td>
        <td>Group Fairness</td>
        <td>Model Testing &amp; Validation</td>
        <td>Model-Agnostic</td>
        <td>Ensuring equal acceptance rates in college admissions across
          demographics.</td>
      </tr>
      <tr class="odd">
        <td><strong>Reject Option Classification</strong></td>
        <td>Changes decisions where the model is least certain, favouring the
          disadvantaged group within this uncertain region.</td>
        <td>Post-Processing Techniques</td>
        <td>Outcome Adjustment</td>
        <td>Group Fairness</td>
        <td>Model Testing &amp; Validation</td>
        <td>Model-Agnostic</td>
        <td>Mitigating bias in hiring decisions by adjusting uncertain
          predictions.</td>
      </tr>
      <tr class="even">
        <td><strong>Calibration with Equality of Opportunity</strong></td>
        <td>Adjusts probabilities to achieve equal true positive rates across
          groups while maintaining calibration within each group.</td>
        <td>Post-Processing Techniques</td>
        <td>Calibration Methods</td>
        <td>Group Fairness</td>
        <td>Model Testing &amp; Validation</td>
        <td>Model-Agnostic</td>
        <td>Balancing opportunity in credit scoring across different ethnic
          groups.</td>
      </tr>
    </tbody>
  </table>
</body>

</html>