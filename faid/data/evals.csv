Evaluation Metrics, Descriptions, Method, Perturbation Fairness
Mean win rate, How many models this model outperform on average (over columns),
MMLU - EM, The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al. 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ)., Exact match: Fraction of instances that the predicted output matches a correct reference exactly., Computes worst case over different fairness perturbations (changing dialect/race of names/gender).
BoolQ - EM, The BoolQ benchmark for binary (yes/no) question answering [(Clark et al. 2019)](https://aclanthology.org/N19-1300/)., Quasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing., Computes worst case over different fairness perturbations (changing dialect/race of names/gender).
NarrativeQA - F1, The NarrativeQA benchmark for reading comprehension over narratives [(Kočiský et al. 2017)](https://aclanthology.org/Q18-1023/)., F1: Average F1 score in terms of word overlap between the model output and correct reference., Computes worst case over different fairness perturbations (changing dialect/race of names/gender).
NaturalQuestions (closed), The NaturalQuestions [(Kwiatkowski et al. 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input does not include the Wikipedia page with the answer., F1: Average F1 score in terms of word overlap between the model output and correct reference., Computes worst case over different fairness perturbations (changing dialect/race of names/gender).
NaturalQuestions (open), The NaturalQuestions [(Kwiatkowski et al. 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input includes the Wikipedia page with the answer., F1: Average F1 score in terms of word overlap between the model output and correct reference., Computes worst case over different fairness perturbations (changing dialect/race of names/gender).
QuAC - F1, The QuAC benchmark for question answering in the context of dialogues [(Choi et al. 2018)](https://aclanthology.org/D18-1241/)., F1: Average F1 score in terms of word overlap between the model output and correct reference. ,Computes worst case over different fairness perturbations (changing dialect/race of names/gender).
HellaSwag - EM, The HellaSwag benchmark for commonsense reasoning in question answering [(Zellers et al. 2019)](https://aclanthology.org/P19-1472/)., Exact match: Fraction of instances that the predicted output matches a correct reference exactly. ,Computes worst case over different fairness perturbations (changing dialect/race of names/gender).
OpenbookQA - EM, The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al. 2018)](https://aclanthology.org/D18-1260/)., Exact match: Fraction of instances that the predicted output matches a correct reference exactly., Computes worst case over different fairness perturbations (changing dialect/race of names/gender).
TruthfulQA - EM, The TruthfulQA benchmarking for measuring model truthfulness and commonsense knowledge in question answering [(Lin et al. 2022)](https://aclanthology.org/2022.acl-long.229/)., Exact match: Fraction of instances that the predicted output matches a correct reference exactly., Computes worst case over different fairness perturbations (changing dialect/race of names/gender).
MS MARCO (regular) - RR@10, The MS MARCO benchmark's regular track for passage retrieval in information retrieval [(https://microsoft.github.io/msmarco/)](https://microsoft.github.io/msmarco/)., RR@10: Mean reciprocal rank at 10 in information retrieval., Computes worst case over different fairness perturbations (changing dialect/race of names/gender).
MS MARCO (TREC) - NDCG@10, The MS MARCO benchmark's deep learning TREC track for passage retrieval in information retrieval [(https://trec.nist.gov)](https://microsoft.github.io/msmarco/)., NDCG@10: Normalized discounted cumulative gain at 10 in information retrieval., Computes worst case over different fairness perturbations (changing dialect/race of names/gender).
IMDB - EM, The IMDB benchmark for sentiment analysis in movie review [(Maas et al. 2011)](https://aclanthology.org/P11-1015/)., Quasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing., Computes worst case over different fairness perturbations (changing dialect/race of names/gender).
CivilComments - EM, The CivilComments benchmark for toxicity detection [(Borkan et al. 2019)](https://arxiv.org/pdf/1903.04561.pdf)., Quasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing., Computes worst case over different fairness perturbations (changing dialect/race of names/gender).
RAFT - EM, The Real-world annotated few-shot (RAFT) meta-benchmark of 11 real-world text classification tasks [(Alex et al. 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ca46c1b9512a7a8315fa3c5a946e8265-Abstract-round2.html)., Quasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing., Computes worst case over different fairness perturbations (changing dialect/race of names/gender).