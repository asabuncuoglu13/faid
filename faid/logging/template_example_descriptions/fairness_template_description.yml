id: "1"
name: "Fairness Analysis Report"  # Name of the report
context:
  authors: ["John Doe", "Jane Smith"]  # List of authors
  start_time: "2023-10-01T12:00:00Z"  # Start time of the analysis
  description: "This report analyzes the fairness of the model."  # Description of the report
  tags: ["fairness", "bias", "ML"]  # Tags related to the report
  hardware: 
    cpu: "Intel i7"
    gpu: "NVIDIA GTX 1080"
    ram: "16GB"  # Hardware specifications used for the analysis
  license_info: "MIT"  # License information
data:
  sample: ["sample1.csv", "sample2.csv"]  # Sample data files used
  variable_profile: "profile.json"  # Profile of variables
  protected_characteristics: ["gender", "race"]  # Protected characteristics considered
  nominal_features: ["feature1", "feature2"]  # Nominal features in the data
  continuous_features: ["feature3", "feature4"]  # Continuous features in the data
model:
  name: "Fairness Model v1"  # Name of the model
sample_data:
  tps: [100, 120]  # True positives
  fps: [10, 15]  # False positives
  tns: [80, 90]  # True negatives
  fns: [5, 7]  # False negatives
bias_metrics:
  groups:
    group_name: "Gender"  # Name of the group
    description: "Bias metrics for gender"  # Description of the group
    label: "gender"  # Label for the group
    metrics:
      - name: "Disparate Impact"
        description: "Ratio of outcomes for different groups"
        value: 0.8
        threshold: 0.7
        bigger_is_better: "false"
        label: "DI"
        notes: "Acceptable range is 0.8 to 1.25"
        sg_params: {"param1": "value1"}  # Additional parameters for subgroup analysis